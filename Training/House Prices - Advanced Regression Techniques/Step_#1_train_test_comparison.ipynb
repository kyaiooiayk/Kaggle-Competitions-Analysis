{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What?** Kaggle competition: House Prices - Advanced Regression Techniques. This particular notebook serves as a common repository to code snippets of my own or taken from other kagglers.\n",
    "\n",
    "- **Dataset description** Ask a home buyer to describe their dream house, and they probably won’t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition’s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General info\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To promote code reusability and tidiness, I will try to bring inside a method most of the actions performed in this notebook.\n",
    "- If you do not like it, it would extremely easy to get rid of the method and use the content as a code snippet.\n",
    "- Please, consider this notebook as a collection of ideas taken (and made mine with some modifications) from several notebooks published by other kagglers who generously shared their idea. Here I am returning the favour for the benefit of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook is part 1 of a 4-series analysis:\n",
    "    - **Step_#1_Train_test_comparison.ipynb**\n",
    "    - Step_#2_EDA.ipynb\n",
    "    - Step_#3_Data_preparation.ipynb\n",
    "    - Step_#4_Modelling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dara wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import copy\n",
    "from functools import reduce\n",
    "import pandas_profiling as pp\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other notebook settings\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display as dsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load datasets\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To know what this step does read the comments inside the `load_data` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load data\n",
    "\n",
    "    Load the train and test data as provided by Kaggle.\n",
    "    Keep in mind that the way Kaggle provides the data is\n",
    "    different than the usual idea we have of the trian-test\n",
    "    split. In particular, the target column is not present\n",
    "    in the test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train : pandas dataframe\n",
    "    test : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nLoading data\")\n",
    "\n",
    "    # Read the train data\n",
    "    print(\"Read train set\")\n",
    "    train = pd.read_csv('./DATASETS/train.csv')\n",
    "\n",
    "    # Read the test data\n",
    "    print(\"Read test set\")\n",
    "    test = pd.read_csv('./DATASETS/test.csv')\n",
    "\n",
    "    print(\"Train size\", train.shape)\n",
    "    print(\"Test size:\", test.shape)\n",
    "\n",
    "    train_features = train.columns\n",
    "    test_features = test.columns\n",
    "    print(\"Not share columns: \", set(train_features).difference(test_features))\n",
    "    print(\"Not share columns: \", set(test_features).difference(train_features))\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read data for the first time\n",
    "train, test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Get the number of categorical and numerical features.\n",
    "- This information can be used to debug the final dataset.\n",
    "- The difference of (-1) between numerical features between train and set is due to the fact that the test set provided by Kaggle does not have the target column. The idea is that the target in the test set is then used to score your submission in the public board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_features_type(SET):\n",
    "    \n",
    "    print(\"\\nGet feature types\")\n",
    "    \n",
    "    df_numerical_features = SET.select_dtypes(exclude=['object'])\n",
    "    df_non_numerical_features = SET.select_dtypes(include=['object'])\n",
    "    \n",
    "    print(\"No of numerical features: \", df_numerical_features.shape)\n",
    "    print(\"No of NON numerical features: \", df_non_numerical_features.shape)    \n",
    "        \n",
    "    return df_numerical_features, df_non_numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "_,_ = get_features_type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "_,_ = get_features_type(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Check for duplicates in both train and test set,\n",
    "- The check is done against the ID but a more thourough one would involve checking if different ID have exactly the same entry for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ids_dupli_train = train.shape[0] - len(train[\"Id\"].unique())\n",
    "Ids_dupli_test = test.shape[0] - len(test[\"Id\"].unique())\n",
    "\n",
    "print(\"There are \" + str(Ids_dupli_train) + \" duplicated IDs for \" +\n",
    "      str(train.shape[0]) + \" total TRAIN entries\")\n",
    "print(\"There are \" + str(Ids_dupli_test) + \" duplicated IDs for \" +\n",
    "      str(test.shape[0]) + \" total TEST entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minor changes\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just a collection of actions generally forgotten. It is better to do here so we'll keep the notebook clean for later steps.\n",
    "- More info is provided under the `minor_changes` doc string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def minor_changes(train, test, target_name):\n",
    "    \"\"\"Minor changes.\n",
    "\n",
    "    This method performs minor changes to the dataset.\n",
    "    These are generally forgotten actions hence the\n",
    "    name of the method.\n",
    "\n",
    "    At the moment we have:\n",
    "    - Get train and test IDs, the former used for Kaggle\n",
    "    competition submission file\n",
    "    - Remove the column Id as it is not needed. This is an \n",
    "    artifact used by Kaggle to keep track of the submitted\n",
    "    predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pandas datafrme\n",
    "    test : pandas dataframe\n",
    "    target_name : string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train : pandas datafrme\n",
    "        Dataframe NOT containing only the ID column\n",
    "    test : pandas dataframe\n",
    "        Dataframe NOT containing only the ID column\n",
    "    df_target : pandas dataframe\n",
    "        Dataframe containing only the target    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Train size BEFORE:\", train.shape)\n",
    "    print(\"Test size BEFORE:\", test.shape)\n",
    "\n",
    "    Id_train = train.Id.values\n",
    "    Id_test = test.Id.values\n",
    "\n",
    "    if test['Id'].count() != 0.0:\n",
    "        print(\"Removing column Id from test set\")\n",
    "        test = test.drop(['Id'], axis=1)\n",
    "    else:\n",
    "        print(\"No column ID present\")\n",
    "\n",
    "    if train['Id'].count() != 0.0:\n",
    "        print(\"Removing column Id from train set\")\n",
    "        train = train.drop(['Id'], axis=1)\n",
    "    else:\n",
    "        print(\"No column ID present\")\n",
    "\n",
    "    print(\"Train size AFTER:\", train.shape)\n",
    "    print(\"Test size AFTER:\", test.shape)\n",
    "    df_target = train[target_name]\n",
    "\n",
    "    return train, test, Id_train, Id_test, df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, Id_train, Id_test, TARGET = minor_changes(train, test, \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quick and comprehensive data overview with Pandas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pandas has a little known profiler which automatically prints a complete and comprehensive description of your data.\n",
    "- It can be used as a reminder of what you can try to build in terms of systematic data profiler and also as a quick overview of what you should really pay attention to.\n",
    "- **Just a small warning**: the size of the notebook will increase cosniderably (~60MB), so you may want to comment out these two lines of code if size/loading is an issue!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comment this if notebook size/loading is an issue!\n",
    "#pp.ProfileReport(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Comment this if notebook size/loading is an issue!\n",
    "#pp.ProfileReport(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train vs. test sets\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this section we are going to compare the train against the test sets. This is somethng that is generally not done, but I feel comparing the two would help us understand more about the set and how it was split.\n",
    "- It is generally assumed that train and test are a representative of the same population. This is a **naive assumption** as there is very little guarantee this is the case. I feel (just speculatio here) that this way of working is that in a Kaggle competition even if you are aware of such a difference, the only thing you can do is simply be aware of it and **more importabtly** be weary of the score you get on the **public board**. Essentially, we are asking whether the test set is statistically representative of the train set or not? \n",
    "- **Why are we doing this?** It has been reported that, in some cases, the private set was not representative of the one used for scoring submissions on the leaderboard. If that was a possibility in the past, then it is fair to assume that the the same can be thought of the train and test set provided by Kaggle. Whether this is a necessary step or not it is arguable, but it'll help us understand the data. The only shared notebook I've found covering something similar is [this one](https://www.kaggle.com/gunesevitan/house-prices-advanced-stacking-tutorial)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def basic_details(df, sort_by=\"Feature\"):\n",
    "    \"\"\"Get basic details of the dataset.\n",
    "\n",
    "    The following feature are recorded:\n",
    "        [1] Missing values and their percentage\n",
    "        [2] Unique values and their percentage (Cardinality)\n",
    "            Cardinality is important if you are trying to \n",
    "            understand feature importance.\n",
    "        [3] Type if numerical or not    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "    sort_by : string, defaul = \"Feature\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    b : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    b = pd.DataFrame()\n",
    "    b['No missing value'] = df.isnull().sum()\n",
    "    b[\"Missing[%]\"] = df.isna().mean()*100\n",
    "    b['No unique value'] = df.nunique()\n",
    "    b['Cardinality[%]'] = (df.nunique()/len(df.values))*100\n",
    "    b[\"No Values\"] = [len(df.values) for _ in range(len(df.columns))]\n",
    "    b['dtype'] = df.dtypes\n",
    "\n",
    "    # Turn index into a columns\n",
    "    b['Feature'] = b.index\n",
    "    # Getting rid of the index\n",
    "    b.reset_index(drop=True, inplace=True)\n",
    "    # Order by feature name\n",
    "    b.sort_values(by=[sort_by], inplace=True)\n",
    "    # Move feature as first column\n",
    "    b = b[['Feature'] + [col for col in b.columns if col != 'Feature']]\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basic_details(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_details(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'd like to know the number of **non usable entries**. Non usable entries are here defined as entries that cannot be used directly. This means that, in some cases, we can impute the data and use a valid value instead.\n",
    "- This is anothr occasion to check if the splitting provided is representative of the real data distribution or not. It is not so rare to be given a test set whose property are not *similar* to the train set.\n",
    "- We'll then highlight all those feature for which the difference in percetnage is greater than a threshold of your choice. I am not sure what that threshold would be, but I am assuming a value around 2% would be a sensible choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compare_sets_over_non_usable_entries(train, test, delta_threshold=2.0):\n",
    "    \"\"\"Compare sets over non usable entries\n",
    "\n",
    "    As the name suggests two sets are compared over their numbers\n",
    "    of non-usable entries. If the percentage difference is greater\n",
    "    than 2%, this is then highlighted.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pandas dataframe\n",
    "    test : pandas dataframe\n",
    "    delta_threshold : float, default = 2.0\n",
    "         Value in percentage above which the column get highlithed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    b : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Pandas dataframe showing the number of null values for the train set\n",
    "    nan_train = pd.DataFrame(train.isna().sum(), columns=['Nan_sum_train'])\n",
    "    nan_train['feature_name'] = nan_train.index\n",
    "    nan_train = nan_train[nan_train['Nan_sum_train'] > 0]\n",
    "    nan_train['Percentage_train'] = (nan_train['Nan_sum_train']/len(train))*100\n",
    "    nan_train = nan_train.sort_values(by=['feature_name'])\n",
    "\n",
    "    # Pandas dataframe showing the number of null values for the test set\n",
    "    nan_test = pd.DataFrame(test.isna().sum(), columns=['Nan_sum_test'])\n",
    "    nan_test['feature_name'] = nan_test.index\n",
    "    nan_test = nan_test[nan_test['Nan_sum_test'] > 0]\n",
    "    nan_test['Percentage_test'] = (nan_test['Nan_sum_test']/len(test))*100\n",
    "    nan_test = nan_test.sort_values(by=['feature_name'])\n",
    "\n",
    "    # Merge the two datasets by \"feature_name\"\n",
    "    pd_merge = pd.merge(nan_test, nan_train, how='outer', on='feature_name')\n",
    "    pd_merge = pd_merge.fillna(0)\n",
    "    pd_merge[\"NaN_tot\"] = pd_merge[\"Nan_sum_train\"] + pd_merge[\"Nan_sum_test\"]\n",
    "    pd_merge[\"delta_percentage\"] = abs(\n",
    "        pd_merge[\"Percentage_test\"] - pd_merge[\"Percentage_train\"])\n",
    "    pd_merge = pd_merge.sort_values(by=['feature_name'])\n",
    "\n",
    "    # We'd like to highlight those entries where the differences > delta_threshold\n",
    "    def highlight(x):\n",
    "        return ['background: yellow' if v > delta_threshold else '' for v in x]\n",
    "\n",
    "    def bold(x):\n",
    "        return ['font-weight: bold' if v > delta_threshold else '' for v in x]\n",
    "\n",
    "    # Highligth the entries\n",
    "    a = pd_merge.style.apply(highlight, subset=\"delta_percentage\").apply(\n",
    "        bold, subset=\"delta_percentage\")\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    " compare_sets_over_non_usable_entries(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The barplot below shows how the No of NaN compare against the features for both training and test set.\n",
    "- Further we can see how the number of NaN is pretty similar for both sets. Nevertheless we can still capture some small deviatios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_sets_over_non_usable_entries(train, test):\n",
    "    \"\"\"Plot sets over non usable entries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pandas dataframe\n",
    "    test : pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    \n",
    "    # Pandas dataframe showing the number of null values for the train set\n",
    "    nan_train = pd.DataFrame(train.isna().sum(), columns=['Nan_sum_train'])\n",
    "    nan_train['feature_name'] = nan_train.index\n",
    "    nan_train = nan_train[nan_train['Nan_sum_train'] > 0]\n",
    "    nan_train['Percentage_train'] = (nan_train['Nan_sum_train']/len(train))*100\n",
    "    nan_train = nan_train.sort_values(by=['feature_name'])    \n",
    "\n",
    "    # Pandas dataframe showing the number of null values for the test set\n",
    "    nan_test = pd.DataFrame(test.isna().sum(), columns=['Nan_sum_test'])\n",
    "    nan_test['feature_name'] = nan_test.index\n",
    "    nan_test = nan_test[nan_test['Nan_sum_test'] > 0]\n",
    "    nan_test['Percentage_test'] = (nan_test['Nan_sum_test']/len(test))*100\n",
    "    nan_test = nan_test.sort_values(by=['feature_name'])    \n",
    "\n",
    "    # Merge the two dataset by \"feature_name\"\n",
    "    pd_merge = pd.merge(nan_test, nan_train, how='outer', on='feature_name')\n",
    "    pd_merge = pd_merge.fillna(0)\n",
    "    pd_merge[\"NaN_tot\"] = pd_merge[\"Nan_sum_train\"] + pd_merge[\"Nan_sum_test\"]\n",
    "    pd_merge[\"delta_percentage\"] = abs(\n",
    "        pd_merge[\"Percentage_test\"] - pd_merge[\"Percentage_train\"])\n",
    "    pd_merge = pd_merge.sort_values(by=['feature_name'])\n",
    "\n",
    "    # Plotting\n",
    "    rcParams['figure.figsize'] = 19, 8\n",
    "    rcParams['font.size'] = 15\n",
    "    pd_merge = pd_merge.sort_values(by=['feature_name'])\n",
    "    plt.figure()\n",
    "    labels = [\"train\", \"test\"]\n",
    "\n",
    "    sns.barplot(x=pd_merge['feature_name'],\n",
    "                y=pd_merge['Percentage_train'], linewidth=2.5, facecolor=\"w\",\n",
    "                errcolor=\".2\", edgecolor=\"k\",)\n",
    "\n",
    "    ax = sns.barplot(x=pd_merge['feature_name'],\n",
    "                     y=pd_merge['Percentage_test'], linewidth=2.5, facecolor=\"w\",\n",
    "                     errcolor=\".2\", edgecolor=\"r\", ls=\"--\")\n",
    "\n",
    "    plt.xticks(rotation=90, size=25)\n",
    "    plt.title('Train vs. test sets', size=25)\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('% of Missing Data', size=25)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_sets_over_non_usable_entries(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Are the difference in percetage shown above signigicant?** There is a more sophisticated way to check if the differences in distribution are really significant. The test is called `t-test`. We can only run this test for numerical variables and we also have to get rid of the null values. At the moment I am turning each null value into zero, but I am unsure on how to best handle this case. [See this discussion](https://www.researchgate.net/post/How-to-handle-missing-data-for-a-paired-t-test)\n",
    "- In this particular case we can see that only three features have a statistically different dsitribution. **What can we do about it?** Considering that this is a competition and you are NOt in the position to change the dataset, you can only be aware of this fact and that's all!\n",
    "- Further we'll also check if any of the instances in the test set extend over the domain of the train set. If so, we are training a model that would not be able to predict the test because the model has not seen those values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_IQR_frequency(df):\n",
    "    \"\"\"Get the IQR or the frequency.\n",
    "    \n",
    "    This method returns the interquartilies or the frquenecy\n",
    "    depending on the feature beeing numerical or categorical.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dummy : pandas dataframe\n",
    "        dummy storing IQR if numerical\n",
    "        dummy storing the instance frequency if categorical\n",
    "    \"\"\"\n",
    "\n",
    "    if all(df.dtypes != object):\n",
    "        dummy = pd.DataFrame(df.describe()).T\n",
    "    else:\n",
    "        frequency = []\n",
    "        frequency_percentage = []\n",
    "        unique = list(set([i[0] for i in pd.DataFrame(df).values]))\n",
    "        for i in unique:\n",
    "            frequency.append(df[df == i].count()[0])\n",
    "            frequency_percentage.append((df[df == i].count()[0]/len(df))*100)\n",
    "\n",
    "        dummy = pd.DataFrame()\n",
    "        dummy[\"Entries\"] = unique\n",
    "        dummy[\"Frequency\"] = frequency\n",
    "        dummy[\"Frequency[%]\"] = frequency_percentage\n",
    "        dummy.sort_values(by=['Frequency[%]'], inplace=True, ascending=False)\n",
    "\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compare_distribution_sets_on_numerical_columns(train, test):\n",
    "    \"\"\"Compare feature distribution on both train and test set.\n",
    "    \n",
    "    Test if each feature has the same distribution\n",
    "    in both train and test set. This is achieved via t-test. \n",
    "    To be able to perform this test we ONLY select the numerical \n",
    "    variables.\n",
    "          \n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pandas dataframe\n",
    "    test : pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    dummy : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # The reason why we get the test features is because we are sure\n",
    "    # that every single feature is present in the train set and not viceversa\n",
    "    # in fact, the target is not present in the test set\n",
    "    numeric_features = test.dtypes[test.dtypes != object].index\n",
    "\n",
    "    similar = []\n",
    "    p_value = []\n",
    "    mu, sigma = [], []\n",
    "    min_test_within_train = []\n",
    "    max_test_within_train = []\n",
    "    \n",
    "    for feature in numeric_features:\n",
    "\n",
    "        # Getting rid of all null values. We are using zero as a form of imputation\n",
    "        # but this was not a thought process in the sense that if you do not use it\n",
    "        # throws you an error.\n",
    "        train_clean = train[feature].fillna(0.0)\n",
    "        test_clean = test[feature].fillna(0.0)\n",
    "\n",
    "        stat, p = ttest_ind(train_clean, test_clean) \n",
    "        p_value.append(p)\n",
    "\n",
    "        #print('Statistics=%.3f, p=%.3f' % (stat, p))\n",
    "        alpha = 0.05\n",
    "        if p > alpha:\n",
    "            similar.append(\"similar\")                \n",
    "            #print('Same distributions (fail to reject H0)')\n",
    "        else:\n",
    "            similar.append(\"different\")\n",
    "            #print('Different distributions (reject H0)')\n",
    "                \n",
    "        min_train = get_IQR_frequency(pd.DataFrame(train_clean))[\"min\"].values[0]\n",
    "        min_test = get_IQR_frequency(pd.DataFrame(test_clean))[\"min\"].values[0]\n",
    "        \n",
    "        max_train = get_IQR_frequency(pd.DataFrame(train_clean))[\"max\"].values[0]\n",
    "        max_test = get_IQR_frequency(pd.DataFrame(test_clean))[\"max\"].values[0]\n",
    "        \n",
    "        #print(\"----\",max_test <= max_train)\n",
    "        \n",
    "        min_test_within_train.append(min_test >= min_train)\n",
    "        max_test_within_train.append(max_test <= max_train)\n",
    "        \n",
    "\n",
    "    # Create a pandas dataframe\n",
    "    dummy = pd.DataFrame()\n",
    "    dummy[\"numerical_feature\"] =  numeric_features\n",
    "    dummy[\"type\"] = [\"numerical\" for _ in range(len(numeric_features))]\n",
    "    dummy[\"train_test_similar?\"] = similar\n",
    "    dummy[\"ttest_p_value\"] = p_value\n",
    "    dummy[\"min_test>=min_train\"] = min_test_within_train\n",
    "    dummy[\"max_test<=max_train\"] = max_test_within_train\n",
    "\n",
    "    # Decorate the dataframe for quick visualisation\n",
    "    def highlight(x):    \n",
    "        return ['background: yellow' if v == \"different\" or v == False else '' for v in x]\n",
    "\n",
    "    def bold(x):\n",
    "        return ['font-weight: bold' if v == \"different\" or v == False else '' for v in x]\n",
    "\n",
    "    # Visualise the highlighted df\n",
    "    return dummy.style.apply(highlight).apply(bold)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_distribution_sets_on_numerical_columns(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One of the things we can do is to visually compare (via an histograma and its kde) the distribution of each feature over the two sets.\n",
    "- This is much easier to read but it would not be as precise as the T-test described above.\n",
    "- Also consider that we have not transformed the data as yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_unique_values(df):\n",
    "    \"\"\"Get unique values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    unique : set\n",
    "    \"\"\"\n",
    "\n",
    "    unique = set([i[0] for i in df.dropna().values])\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compare_hist_kde(train, test):\n",
    "    \"\"\"Compare hist and kde for two histogram\n",
    "\n",
    "    Parameteres\n",
    "    -----------\n",
    "    train : pandas dataframe\n",
    "    test : pandas dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None    \n",
    "    \"\"\"\n",
    "    rcParams['figure.figsize'] = 17, 5\n",
    "    rcParams['font.size'] = 15\n",
    "\n",
    "    dummy = get_unique_values(train)\n",
    "    No_bins = 50  # int(len(dummy))\n",
    "    print(\"No of bins used for histograme: \", No_bins)\n",
    "\n",
    "    for i in set(list(train.columns.values) + list(test.columns.values)):\n",
    "\n",
    "        print(\"***************\")\n",
    "        print(\"Feature's name:\", i)\n",
    "        print(\"***************\")\n",
    "\n",
    "        # Plot histogram\n",
    "        try:\n",
    "            test[i].hist(legend=True, bins=No_bins)\n",
    "        except:\n",
    "            print(\"Feature\", i, \" NOT present in TEST set!\")\n",
    "        try:\n",
    "            train[i].hist(legend=True, bins=No_bins)\n",
    "        except:\n",
    "            print(\"Feature\", i, \" NOT present in TRAIN set!\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot density function\n",
    "        try:\n",
    "            sns.distplot(test[i], hist=False, kde=True,\n",
    "                         kde_kws={'shade': True, 'linewidth': 3})\n",
    "        except:\n",
    "            print(\"Feature\", i, \" is categorical in TEST set!\")\n",
    "        try:\n",
    "            sns.distplot(train[i], hist=False, kde=True,\n",
    "                         kde_kws={'shade': True, 'linewidth': 3})\n",
    "        except:\n",
    "            print(\"Feature\", i, \" is categorical in TRAIN set!\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "compare_hist_kde(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this notebook we have compared the train and test sets. The reasons for this are many (a non-completed list is reported below):\n",
    "    - We need to know if leakage is a possibility or not. For instance, if I merge the data, am I leaking info from the test to the train set?\n",
    "    - We need to know if the two sets are statistically equivalent, if NOT this will tell us how much we can trust the CV results.\n",
    "    - Knowing the data, especially when it comes to explaining the results, is extremely important.\n",
    "\n",
    "- For this particular dataset:\n",
    "    - Leakage is a risk, however whether this risk will affect the final results or not needs to be established.\n",
    "    - Not all the features have the same distribution in both sets, meaning that there are some statistically relevant differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
