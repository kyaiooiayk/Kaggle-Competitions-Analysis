{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **What?** Kaggle competition: House Prices - Advanced Regression Techniques. This particular notebook serves as a common repository to code snippets of my own or taken from other kagglers.\n",
    "\n",
    "- **Dataset description** Ask a home buyer to describe their dream house, and they probably won’t begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition’s dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence. With 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# General info\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- To promote code reusability and tidiness, I will try to bring inside a method most of the actions performed in this notebook.\n",
    "- If you do not like it, it would extremely easy to get rid of the method and use the content as a code snippet.\n",
    "- Please, consider this notebook as a collection of ideas taken (and made mine with some modifications) from several notebooks published by other kagglers who generously shared their idea. Here I am returning the favour for the benefit of the others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This notebook is part 1 of a 4-series analysis:\n",
    "    - Step_#1_Train_test_comparison.ipynb\n",
    "    - **Step_#2_EDA.ipynb**\n",
    "    - Step_#3_Data_preparation.ipynb\n",
    "    - Step_#4_Modelling.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import modules\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dara wrangling\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import copy\n",
    "from functools import reduce\n",
    "import pandas_profiling as pp\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Features engineering\n",
    "from sklearn.preprocessing import LabelEncoder, OrdinalEncoder\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "# Statistics\n",
    "from scipy.stats import norm\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import spearmanr, kendalltau\n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Other notebook settings\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.float_format', lambda x: '%.5f' % x)\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "from IPython.display import display as dsp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "heading_collapsed": true
   },
   "source": [
    "# Load datasets\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- To know what this step does read the comments inside the `load_data` method.\n",
    "- This is nothing new than what we have seen in `Step_#1_Train_test_comparison.ipynb`. I've just reported the whole section to have a semi self-contained notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def load_data():\n",
    "    \"\"\"Load data\n",
    "\n",
    "    Load the train and test data as provided by Kaggle.\n",
    "    Keep in mind that the way Kaggle provides the data is\n",
    "    different than the usual idea we have of the trian-test\n",
    "    split. In particular, the target column is not present\n",
    "    in the test set.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    None\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train : pandas dataframe\n",
    "    test : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    print(\"\\nLoading data\")\n",
    "    # Read the train data\n",
    "    print(\"Read train set\")\n",
    "    train = pd.read_csv('./DATASETS/train.csv')\n",
    "\n",
    "    # Read the test data\n",
    "    print(\"Read test set\")\n",
    "    test = pd.read_csv('./DATASETS/test.csv')\n",
    "\n",
    "    print(\"Train size\", train.shape)\n",
    "    print(\"Test size:\", test.shape)\n",
    "\n",
    "    train_features = train.columns\n",
    "    test_features = test.columns\n",
    "    print(\"Not share columns: \", set(train_features).difference(test_features))\n",
    "    print(\"Not share columns: \", set(test_features).difference(train_features))\n",
    "\n",
    "    return train, test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "# Read data for the first time\n",
    "train, test = load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Get the number of categorical and numerical features.\n",
    "- This information can be used to debug the final dataset.\n",
    "- The difference of (-1) between numerical features between train and set is due to the fact that the test set provided by Kaggle does not have the target column. The idea is that the target in the test set is then used to score your submission in the public board."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ],
    "hidden": true
   },
   "outputs": [],
   "source": [
    "def get_features_type(SET):\n",
    "\n",
    "    print(\"\\nGet feature types\")\n",
    "\n",
    "    df_numerical_features = SET.select_dtypes(exclude=['object'])\n",
    "    df_non_numerical_features = SET.select_dtypes(include=['object'])\n",
    "\n",
    "    print(\"No of numerical features: \", df_numerical_features.shape)\n",
    "    print(\"No of NON numerical features: \", df_non_numerical_features.shape)\n",
    "\n",
    "    return df_numerical_features, df_non_numerical_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_,_ = get_features_type(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "hidden": true
   },
   "outputs": [],
   "source": [
    "_,_ = get_features_type(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- Check for duplicates in both train and test set,\n",
    "- The check is done against the ID but a more thourough one would involve checking if different ID have exactly the same entry for each column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "hidden": true
   },
   "outputs": [],
   "source": [
    "Ids_dupli_train = train.shape[0] - len(train[\"Id\"].unique())\n",
    "Ids_dupli_test = test.shape[0] - len(test[\"Id\"].unique())\n",
    "\n",
    "print(\"There are \" + str(Ids_dupli_train) + \" duplicate IDs for \" +\n",
    "      str(train.shape[0]) + \" total TRAIN entries\")\n",
    "print(\"There are \" + str(Ids_dupli_test) + \" duplicate IDs for \" +\n",
    "      str(test.shape[0]) + \" total TEST entries\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Minor changes\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Just a collection of actions generally forgotten. It is better to do here so we'll keep the notebook clean for later steps.\n",
    "- More info is provided under the `minor_changes` doc string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def minor_changes(train, test, target_name):\n",
    "    \"\"\"Minor changes.\n",
    "\n",
    "    This method performs minor changes to the dataset.\n",
    "    These are generally forgotten actions hence the\n",
    "    name of the method.\n",
    "\n",
    "    At the moment we have:\n",
    "    - Get train and test IDs, the former used for Kaggle\n",
    "    competition submission file\n",
    "    - Remove the column Id as it is not needed. This is an \n",
    "    artifact used by Kaggle to keep track of the submitted\n",
    "    predictions.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pandas datafrme\n",
    "    test : pandas dataframe\n",
    "    target_name : string\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    train : pandas datafrme\n",
    "        Dataframe NOT containing only the ID column\n",
    "    test : pandas dataframe\n",
    "        Dataframe NOT containing only the ID column\n",
    "    df_target : pandas dataframe\n",
    "        Dataframe containing only the target    \n",
    "    \"\"\"\n",
    "\n",
    "    print(\"Train size BEFORE:\", train.shape)\n",
    "    print(\"Test size BEFORE:\", test.shape)\n",
    "\n",
    "    Id_train = train.Id.values\n",
    "    Id_test = test.Id.values\n",
    "\n",
    "    if test['Id'].count() != 0.0:\n",
    "        print(\"Removing column Id from test set\")\n",
    "        test = test.drop(['Id'], axis=1)\n",
    "    else:\n",
    "        print(\"No column ID present\")\n",
    "\n",
    "    if train['Id'].count() != 0.0:\n",
    "        print(\"Removing column Id from train set\")\n",
    "        train = train.drop(['Id'], axis=1)\n",
    "    else:\n",
    "        print(\"No column ID present\")\n",
    "\n",
    "    print(\"Train size AFTER:\", train.shape)\n",
    "    print(\"Test size AFTER:\", test.shape)\n",
    "    df_target = train[target_name]\n",
    "\n",
    "    return train, test, Id_train, Id_test, df_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train, test, Id_train, Id_test, TARGET = minor_changes(train, test, \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basics details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def basic_details(df, sort_by=\"Feature\"):\n",
    "    \"\"\"Get basic details of the dataset.\n",
    "\n",
    "    Missing values and their percentage\n",
    "    Unique values and their percentage (Cardinality)\n",
    "    Cardinality is important if you are trying to understand\n",
    "    feature importance.\n",
    "    Type if numerical or not    \n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df - pabdas dataframe\n",
    "    sort_by - string, defaul = \"Feature\"\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    b - pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    b = pd.DataFrame()\n",
    "    b['No missing value'] = df.isnull().sum()\n",
    "    b[\"Missing[%]\"] = df.isna().mean()*100\n",
    "    b['No unique value'] = df.nunique()\n",
    "    b['Cardinality[%]'] = (df.nunique()/len(df.values))*100\n",
    "    b[\"No Values\"] = [len(df.values) for _ in range(len(df.columns))]\n",
    "    b['dtype'] = df.dtypes\n",
    "\n",
    "    # Some cosmetic on the table\n",
    "    # Turn index into a columns\n",
    "    b['Feature'] = b.index\n",
    "    # Getting rid of the index\n",
    "    b.reset_index(drop=True, inplace=True)\n",
    "    # Order by feature name\n",
    "    b.sort_values(by=[sort_by], inplace=True)\n",
    "    # Move feature as a first column\n",
    "    b = b[['Feature'] + [col for col in b.columns if col != 'Feature']]\n",
    "\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "basic_details(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_details(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilities to compare two sets\n",
    "<hr style=\"border:2px solid black\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- These are a series of utilies we are going to routinely used while analysis each features.\n",
    "- These will help us keep the code structure tidy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_IQR_frequency(df):\n",
    "    \"\"\"Get the IQR or the frequency.\n",
    "\n",
    "    This method returns the interquartalies or the frquenecy\n",
    "    depending on the feature being numerical or categorical.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dummy : pandas dataframe\n",
    "        dummy storing IQR if numerical\n",
    "        dummy storing each instance frequency if categorical\n",
    "    \"\"\"\n",
    "\n",
    "    if all(df.dtypes != object):\n",
    "        dummy = pd.DataFrame(df.describe()).T\n",
    "    else:\n",
    "        frequency = []\n",
    "        frequency_percentage = []\n",
    "        unique = list(set([i[0] for i in pd.DataFrame(df).values]))\n",
    "        for i in unique:\n",
    "            frequency.append(df[df == i].count()[0])\n",
    "            frequency_percentage.append((df[df == i].count()[0]/len(df))*100)\n",
    "\n",
    "        dummy = pd.DataFrame()\n",
    "        dummy[\"Entries\"] = unique\n",
    "        dummy[\"Frequency\"] = frequency\n",
    "        dummy[\"Frequency[%]\"] = frequency_percentage\n",
    "        dummy.sort_values(by=['Frequency[%]'], inplace=True, ascending=False)\n",
    "\n",
    "    return dummy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compare_distribution_sets_on_numerical_columns(train, test):\n",
    "    \"\"\"Cmpare distribution sets on numerical columns\n",
    "\n",
    "    Test if two feature has the same distribution in both train \n",
    "    and test set. This is achieved via a T-test. To be able to \n",
    "    perform this test we ONLY select the numerical variables, \n",
    "    hence the name of the method.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pandas dataframe\n",
    "    test : pandas dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    dummy : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Test.columns because it does not have the target\n",
    "    numeric_features = test.dtypes[test.dtypes != object].index\n",
    "\n",
    "    similar = []\n",
    "    p_value = []\n",
    "    mu, sigma = [], []\n",
    "    min_test_within_train = []\n",
    "    max_test_within_train = []\n",
    "\n",
    "    for feature in numeric_features:\n",
    "\n",
    "        # Filling the null values with zeros\n",
    "        train_clean = train[feature].fillna(0.0)\n",
    "        test_clean = test[feature].fillna(0.0)\n",
    "\n",
    "        stat, p = ttest_ind(train_clean, test_clean)\n",
    "        p_value.append(p)\n",
    "        \n",
    "        alpha = 0.05\n",
    "        if p > alpha:\n",
    "            similar.append(\"similar\")\n",
    "            #print('Same distributions (fail to reject H0)')\n",
    "        else:\n",
    "            similar.append(\"different\")\n",
    "            #print('Different distributions (reject H0)')\n",
    "\n",
    "        min_train = get_IQR_frequency(pd.DataFrame(train_clean))[\n",
    "            \"min\"].values[0]\n",
    "        min_test = get_IQR_frequency(pd.DataFrame(test_clean))[\"min\"].values[0]\n",
    "\n",
    "        max_train = get_IQR_frequency(pd.DataFrame(train_clean))[\n",
    "            \"max\"].values[0]\n",
    "        max_test = get_IQR_frequency(pd.DataFrame(test_clean))[\"max\"].values[0]        \n",
    "\n",
    "        min_test_within_train.append(min_test >= min_train)\n",
    "        max_test_within_train.append(max_test <= max_train)\n",
    "\n",
    "    # Create a pandas dataframe\n",
    "    dummy = pd.DataFrame()\n",
    "    dummy[\"numerical_feature\"] = numeric_features\n",
    "    dummy[\"type\"] = [\"numerical\" for _ in range(len(numeric_features))]\n",
    "    dummy[\"train_test_similar?\"] = similar\n",
    "    dummy[\"ttest_p_value\"] = p_value\n",
    "    dummy[\"min_test>=min_train\"] = min_test_within_train\n",
    "    dummy[\"max_test<=max_train\"] = max_test_within_train\n",
    "\n",
    "    # Decorate the dataframe for quick visualisation\n",
    "    def highlight(x):\n",
    "        return ['background: yellow' if v == \"different\" or v == False else '' for v in x]\n",
    "    def bold(x):\n",
    "        return ['font-weight: bold' if v == \"different\" or v == False else '' for v in x]\n",
    "    \n",
    "    return dummy.style.apply(highlight).apply(bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_unique_values(df):\n",
    "    \"\"\"Get unique values.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    unique : set\n",
    "    \"\"\"    \n",
    "    \n",
    "    unique = set([i[0] for i in df.dropna().values])\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compare_hist_kde(train, test):\n",
    "    \"\"\"Compare hist and kde\n",
    "    \n",
    "    Parameteres\n",
    "    -----------\n",
    "    train : pandas dataframe\n",
    "    test : pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    None    \n",
    "    \"\"\"\n",
    "    rcParams['figure.figsize'] = 17, 5\n",
    "    rcParams['font.size'] = 15\n",
    "\n",
    "    dummy = get_unique_values(train)\n",
    "    No_bins = 50 #int(len(dummy))\n",
    "    print(\"No of bins used for histograme: \", No_bins)\n",
    "\n",
    "    for i in set(list(train.columns.values) + list(test.columns.values)):\n",
    "        \n",
    "        print(\"***************\")\n",
    "        print(\"Feature's name:\", i)\n",
    "        print(\"***************\")\n",
    "        \n",
    "        # Plot histogram\n",
    "        try:\n",
    "            test[i].hist(legend=True, bins=No_bins)\n",
    "        except:\n",
    "            print(\"Feature\", i, \" NOT present in TEST set!\")\n",
    "        try:\n",
    "            train[i].hist(legend=True, bins=No_bins)\n",
    "        except:\n",
    "            print(\"Feature\", i, \" NOT present in TRAIN set!\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()\n",
    "\n",
    "        # Plot density function\n",
    "        try:\n",
    "            sns.distplot(test[i], hist=False, kde=True,\n",
    "                         kde_kws={'shade': True, 'linewidth': 3})            \n",
    "        except:\n",
    "            print(\"Feature\", i, \" is categorical in TEST set!\")\n",
    "        try:\n",
    "            sns.distplot(train[i], hist=False, kde=True,\n",
    "                         kde_kws={'shade': True, 'linewidth': 3})            \n",
    "        except:\n",
    "            print(\"Feature\", i, \" is categorical in TRAIN set!\")\n",
    "        plt.xticks(rotation=90)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compare_QQ_kde(distr1, distr2):\n",
    "    \"\"\"Compare QQ and KDE plot.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    distr1 : pandas.core.series.Series\n",
    "    distr2 : pandas.core.series.Series\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    fig, ax = plt.subplots(2, 2, figsize=(15, 5))\n",
    "    fig.suptitle(\n",
    "        \" qq-plot & distribution: original vs. log-transform \", fontsize=15)\n",
    "\n",
    "    # SalePrice BEFORE transformation\n",
    "    #sm.qqplot(distr1, stats.t, distargs=(4,), fit=True, line=\"45\", ax=ax[0, 0])\n",
    "    stats.probplot(distr1, plot=ax[0, 0])\n",
    "    sns.distplot(distr1, kde=True, hist=True, fit=norm, ax=ax[0, 1])\n",
    "\n",
    "    # SalePrice AFTER transformation\n",
    "    #sm.qqplot(distr2, stats.t, distargs=(4,), fit=True, line=\"45\", ax=ax[1, 0])\n",
    "    stats.probplot(distr2, plot=ax[1, 0])\n",
    "    sns.distplot(distr2, kde=True, hist=True, fit=norm, ax=ax[1, 1])\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_cat_num_df(df):\n",
    "    \"\"\"Get categorical and numerical features name.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    cat_cols : list\n",
    "    num_cols : list    \n",
    "    \"\"\"\n",
    "\n",
    "    cat_cols = df.select_dtypes(include=['object'])\n",
    "    num_cols = df.select_dtypes(exclude=['object'])\n",
    "    print(\n",
    "        f'The dataset contains {len(cat_cols.columns.tolist())} categorical columns')\n",
    "    print(\n",
    "        f'The dataset contains {len(num_cols.columns.tolist())} numeric columns')\n",
    "\n",
    "    return cat_cols, num_cols"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Target analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- It seems (although the description talks only about RMSE) that the error metric is the RMSE **on the log of the sale prices.**\n",
    "- The target `SalePrice` variable show the following traits:\n",
    "    - Deviate from the normal distribution.\n",
    "    - Have appreciable positive skewness.\n",
    "    - Show peakedness"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "\n",
    "f, ax = plt.subplots(figsize=(16, 5))\n",
    "sns.distplot(train['SalePrice'], color=\"b\")\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "           loc='best', fontsize = 15)\n",
    "\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"SalePrice\")\n",
    "ax.set(title=\"SalePrice distribution\")\n",
    "sns.despine(trim=True, left=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- If the target is **NOT** normally distributed, we can corroborate our intuition by calculating two other measures used in this case:\n",
    "    - Skewness \n",
    "    - Kurtosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Skewness: %f\" % train['SalePrice'].skew())\n",
    "print(\"Kurtosis: %f\" % train['SalePrice'].kurt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The SalePrice is **skewed to the right**, (aka positive skewness).\n",
    "- This is a problem because most ML models don't do well with non-normally distributed data. But bare in mind that other methods, such as trees-based are not affected by this transformation. Since we are spot checking a wide varity of methods, we'll enforce this knowing it is going to benefit only some methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "f, ax = plt.subplots(figsize=(16, 5))\n",
    "\n",
    "# Check the new distribution\n",
    "sns.distplot(np.log1p(train[\"SalePrice\"]), fit=norm, color=\"b\")\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(np.log1p(train['SalePrice']))\n",
    "\n",
    "# Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "           loc='best', fontsize = 15)\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"SalePrice\")\n",
    "ax.set(title=\"SalePrice distribution\")\n",
    "sns.despine(trim=True, left=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this case, `log(1+x)` tranform works fairly well and fixes the skewness. **Why is it so important?** The real advantage is that taking the log **means that errors in predicting expensive houses and cheap houses will affect the result equally**.\n",
    "- We'll log the target after we analyse all the features.\n",
    "- We can compare the two distribution with and withouth the log transform.\n",
    "- The method `compare_QQ_kde` will do just this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "compare_QQ_kde(train[\"SalePrice\"], np.log1p(train[\"SalePrice\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **How about data standardisation?**\n",
    "- Data standardization means converting data values to have mean of 0 and a standard deviation of 1.\n",
    "- This would not make the distribution normal, further some of the value would be negative and that does not make sense."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "saleprice_scaled = StandardScaler().fit_transform(\n",
    "    train['SalePrice'][:, np.newaxis])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"white\")\n",
    "sns.set_color_codes(palette='deep')\n",
    "f, ax = plt.subplots(figsize=(16, 5))\n",
    "#Check the new distribution \n",
    "sns.distplot(saleprice_scaled , fit=norm, color=\"b\");\n",
    "\n",
    "# Get the fitted parameters used by the function\n",
    "(mu, sigma) = norm.fit(train['SalePrice'])\n",
    "print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n",
    "\n",
    "#Now plot the distribution\n",
    "plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n",
    "            loc='best')\n",
    "ax.xaxis.grid(False)\n",
    "ax.set(ylabel=\"Frequency\")\n",
    "ax.set(xlabel=\"SalePrice\")\n",
    "ax.set(title=\"SalePrice distribution\")\n",
    "sns.despine(trim=True, left=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This [reference](https://www.kaggle.com/dgawlik/house-prices-eda) suggested an interesting insight. It is possible that correlations shift with change of SalePrice.\n",
    "- Here houses are divided in two price groups: cheap (under 200000) and expensive. Then means of quantitative variables are compared. \n",
    "- Expensive houses have pools, better overall qual and condition, open porch and increased importance of MasVnrArea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative = [f for f in train.columns if train.dtypes[f] != 'object']\n",
    "quantitative.remove('SalePrice')\n",
    "\n",
    "features = quantitative\n",
    "\n",
    "standard = train[train['SalePrice'] < 200000]\n",
    "pricey = train[train['SalePrice'] >= 200000]\n",
    "\n",
    "diff = pd.DataFrame()\n",
    "diff['feature'] = features\n",
    "diff['difference'] = [(pricey[f].fillna(0.).mean() - standard[f].fillna(0.).mean())/(standard[f].fillna(0.).mean())\n",
    "                      for f in features]\n",
    "\n",
    "rcParams['figure.figsize'] = 17, 8\n",
    "rcParams['font.size'] = 20\n",
    "\n",
    "# Sorting by difference: from low to high\n",
    "diff.sort_values(by=['difference'], inplace=True, ascending=True)\n",
    "\n",
    "sns.barplot(data=diff, x='feature', y='difference')\n",
    "x = plt.xticks(rotation=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Another interesting way of visualising the data was suggested by this [reference](https://www.kaggle.com/janiobachmann/house-prices-useful-regression-techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To understand better our data I will create a category column for SalePrice.\n",
    "train['Price_Range'] = np.nan\n",
    "lst = [train]\n",
    "\n",
    "# Create a categorical variable for SalePrice\n",
    "# I am doing this for further visualizations.\n",
    "for column in lst:\n",
    "    column.loc[column['SalePrice'] < 150000, 'Price_Range'] = 'Low'\n",
    "    column.loc[(column['SalePrice'] >= 150000) & (\n",
    "        column['SalePrice'] <= 300000), 'Price_Range'] = 'Medium'\n",
    "    column.loc[column['SalePrice'] > 300000, 'Price_Range'] = 'High'\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Most outliers are in the high price category nevertheless, in the year\n",
    "# of 2007 saleprice of two houses look extremely high!\n",
    "\n",
    "fig = plt.figure(figsize=(12, 8))\n",
    "ax = sns.boxplot(x=\"YrSold\", y=\"SalePrice\", hue='Price_Range', data=train)\n",
    "plt.title('Detecting outliers', fontsize=16)\n",
    "plt.xlabel('Year the House was Sold', fontsize=14)\n",
    "plt.ylabel('Price of the house', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll drop price as we used this just for visualisation prurpouses\n",
    "train.drop(columns = ['Price_Range'], inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Pronciple component analysis is a technique that will help us understand what is the minimum number of dimensions we can use to describe the differences in this dataset.\n",
    "- This needs to be taken with a pinch of salt as we have not touched the dataset as yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 5))\n",
    "rbst_scaler = RobustScaler()\n",
    "train_rbst = rbst_scaler.fit_transform(train[quantitative].dropna())\n",
    "\n",
    "pca = PCA(36).fit(train_rbst)\n",
    "plt.plot(pca.explained_variance_ratio_.cumsum(), \"k*-\", lw=3)\n",
    "plt.xticks(np.arange(0, 36, 1))\n",
    "plt.xlabel('Number of components', fontweight='bold', size=14)\n",
    "plt.ylabel('Explanined variance ratio', fontweight='bold', size=14)\n",
    "\n",
    "train_pca = PCA(3).fit_transform(train_rbst)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normality test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Skewedness**:\n",
    "\n",
    "    - A skewness of zero or near zero indicates a symmetric distribution.\n",
    "    - A negative value for the skewness indicate a left skewness (tail to the left)\n",
    "    - A positive value for te skewness indicate a right skewness (tail to the right)\n",
    "\n",
    "- **Kurtosis**:\n",
    "    - Kourtosis is a measure of how extreme observations are in a dataset.\n",
    "    - The greater the kurtosis coefficient , the more peaked the distribution around the mean is.\n",
    "    - Greater coefficient also means fatter tails, which means there is an increase in tail risk (extreme results)\n",
    "\n",
    "- Reference: Investopedia: https://www.investopedia.com/terms/m/mesokurtic.asp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def test_set_for_normality(df):\n",
    "    \"\"\"Test det for normality.\n",
    "\n",
    "    Generally the Shapire test is used to check if a distribution\n",
    "    is normal. In practice the shapiro-p value is reported but not used\n",
    "    because it is badly affected if the samples are too large!\n",
    "    We'll use the kurtosis and skew factors to establish if it is\n",
    "    normal or not, but that can be easily be modified.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe    \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df ; pandas dataframe\n",
    "\n",
    "    References\n",
    "    ----------\n",
    "    https://www.researchgate.net/post/P-value-equal-to-0000-in-every-test-how-is-it-possible\n",
    "    \"\"\"\n",
    "\n",
    "    numerical_features = df.dtypes[df.dtypes != object].index\n",
    "\n",
    "    mu, sigma = [], []\n",
    "    normal_pvalue = []\n",
    "    normal = []\n",
    "    skew_all = []\n",
    "    kurtosis_all = []\n",
    "    for feature in numerical_features:\n",
    "        # Use 0.0 for all null values, an alternative could be to drop the row\n",
    "        df_clean = df[feature].fillna(0.0)\n",
    "\n",
    "        shap = stats.shapiro(df_clean)\n",
    "        normal_pvalue.append(shap.pvalue)\n",
    "        skew = df_clean.skew()\n",
    "        skew_all.append(skew)\n",
    "        kurtosis = df_clean.kurt()\n",
    "        kurtosis_all.append(kurtosis)\n",
    "        condition_No1 = skew < 0.5 and skew > -0.5\n",
    "        condition_No2 = kurtosis < 2.0 and kurtosis > -2.0\n",
    "        if condition_No1 and condition_No2:\n",
    "            normal.append(\"normal\")\n",
    "        else:\n",
    "            normal.append(\"not_normal\")\n",
    "\n",
    "    # Create a pandas dataframe\n",
    "    dummy = pd.DataFrame()\n",
    "    dummy[\"numerical_feature\"] = numerical_features\n",
    "    dummy[\"type\"] = [\"numerical\" for _ in range(len(numerical_features))]\n",
    "    dummy[\"shapiro_pvalue\"] = normal_pvalue\n",
    "    dummy[\"skew\"] = skew_all\n",
    "    dummy[\"kurtosis\"] = kurtosis_all\n",
    "    dummy[\"normal?\"] = normal\n",
    "\n",
    "    # Decorate the dataframe for quick visualisation\n",
    "    def highlight(x):\n",
    "        return ['background: yellow' if v == \"different\" or v == \"not_normal\" else '' for v in x]\n",
    "\n",
    "    def bold(x):\n",
    "        return ['font-weight: bold' if v == \"different\" or v == \"not_normal\" else '' for v in x]\n",
    "\n",
    "    dummy.sort_values(by=['numerical_feature'], inplace=True, ascending=True)\n",
    "    \n",
    "    # Visualise the highlighted df\n",
    "    return dummy.style.apply(highlight).apply(bold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set_for_normality(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_set_for_normality(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Low variability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We have seen how some features mostly consist of just a single value or 0s, which is not useful to us. \n",
    "- Therefore, we set an user-defined threshold of **96%**. If a column has one entry which represents 96% of the entries, then it is reasonable to think the features to be useless since there isnt much information we can extract from it.\n",
    "- This feature are often called `overfitting feature` because the fact that they have low cardinality will make too easy for the algorithm to **overfit them** as suggested in this [notebook](https://www.kaggle.com/angqx95/data-science-workflow-top-2-with-tuning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get numerical and categorical features\n",
    "cat_cols_train, num_cols_train = get_cat_num_df(train)\n",
    "cat_cols_test, num_cols_test = get_cat_num_df(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_low_cardinality_features(df, threshold, feature_names):\n",
    "    \"\"\"Get low cardinality features\n",
    "\n",
    "    Parameters\n",
    "    ---------\n",
    "    df : pandas dataframe\n",
    "    threshold : float \n",
    "        float representing a threshold\n",
    "    feature_names : list of strings\n",
    "        list of strings containing the name of the\n",
    "        features to be checked\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    selected_features = []\n",
    "    for i in feature_names:\n",
    "        #print(i, counts, counts.iloc[0], counts.iloc[1])\n",
    "        # stop\n",
    "        counts = df[i].value_counts()\n",
    "        # print(counts)\n",
    "\n",
    "        # How many zeros?\n",
    "        instance_count = [counts.iloc[i] for i in range(len(counts))]\n",
    "        # print(instance_count)\n",
    "        instance_percentage = [i / len(train) * 100 for i in instance_count]\n",
    "        # print(instance_percentage)\n",
    "        if any([i > float(threshold) for i in instance_percentage]):\n",
    "            #print(\"found, \", i, instance_percentage)\n",
    "            selected_features.append(i)\n",
    "\n",
    "    print(\"Features for which at least one of its entry has cardinality as low as\\n\", str(\n",
    "        100-threshold), \"%\", selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cardinality on numerical features\n",
    "get_low_cardinality_features(train, 96, num_cols_train.columns)\n",
    "get_low_cardinality_features(test, 96, num_cols_test.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check cardinality on categorical features\n",
    "get_low_cardinality_features(train, 96, cat_cols_train.columns)\n",
    "get_low_cardinality_features(test, 96, cat_cols_test.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Two comments:\n",
    "    - There seems to be a consistency between both train and test set.\n",
    "    - Although we have made the argument that this feature should be dropped,, we still have not check for null values and we made no imputation on the dataset. Thus, we'll keep this analysis into consideration and come back to double check if this statement still holds after the cleaning and imputations steps. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This section concentrates on some basic analysis for each feature. \n",
    "- It offers an occasion to study each single variable in details. \n",
    "- This is a fundamental step for the subsequent feature engineer step.\n",
    "- The comments/actions made/taken in this analysis are then implemented either under the imputation or the feature engineering sections.\n",
    "- The main goal of the `analyse_single_feature` method is to collect everything a Data Scientist needs to know on each single feature. **Essentially, can we produce a systematic (read it -> semi-automatic) series of plots that will drive the DS decisions?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- *Are train and test sets representative of the same data?*\n",
    "    - **Scenario No#1** - An entry is present in the TRAIN but not in the TEST set. The issue was on how the data was splitted. This is a red flag that should tell you that probably the private leader board could suffer from the same issue. \n",
    "    - **Scenario No#2** - An entries is present in the TEST but not in the TRAIN set. This is even a worse case scenario, as the model we'll see no correlation as there were not data to train on. The issue was on how the data was splitted. \n",
    "- *What to do if one feature is NOT distributed uniformerly between the train and test sets?* As the data is provided directly from kaggle, you cannot really do anything about it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compare_sets_over_non_usable_entries(train, test, delta_threshold=2.0):\n",
    "    \"\"\"Compare sets over non usable entries\n",
    "\n",
    "    As the name suggests two sets are compared over their\n",
    "    of non-usable entries. If the percentage difference is greater\n",
    "    than the user-defined threshold, the feature is then highlighted.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    train : pandas dataframe\n",
    "    test : pandas dataframe\n",
    "    delta_threshold : float, default = 2.0\n",
    "         Value in percentage above which the row get highlithed\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    b : pandas dataframe\n",
    "    \"\"\"\n",
    "\n",
    "    # Pandas dataframe showing the number of null values for the train set\n",
    "    nan_train = pd.DataFrame(train.isna().sum(), columns=['Nan_sum_train'])\n",
    "    nan_train['feature_name'] = nan_train.index\n",
    "    nan_train = nan_train[nan_train['Nan_sum_train'] > 0]\n",
    "    nan_train['Percentage_train'] = (nan_train['Nan_sum_train']/len(train))*100\n",
    "    nan_train = nan_train.sort_values(by=['feature_name'])    \n",
    "\n",
    "    # Pandas dataframe showing the number of null values for the test set\n",
    "    nan_test = pd.DataFrame(test.isna().sum(), columns=['Nan_sum_test'])\n",
    "\n",
    "    nan_test['feature_name'] = nan_test.index\n",
    "    nan_test = nan_test[nan_test['Nan_sum_test'] > 0]\n",
    "    nan_test['Percentage_test'] = (nan_test['Nan_sum_test']/len(test))*100\n",
    "    nan_test = nan_test.sort_values(by=['feature_name'])    \n",
    "\n",
    "    # Merge the two dataset by \"feature_name\"    \n",
    "    pd_merge = pd.merge(nan_test, nan_train, how='outer', on='feature_name')\n",
    "    pd_merge = pd_merge.fillna(0)\n",
    "    pd_merge[\"NaN_tot\"] = pd_merge[\"Nan_sum_train\"] + pd_merge[\"Nan_sum_test\"]\n",
    "    pd_merge[\"delta_percentage\"] = abs(\n",
    "        pd_merge[\"Percentage_test\"] - pd_merge[\"Percentage_train\"])\n",
    "    pd_merge = pd_merge.sort_values(by=['feature_name'])\n",
    "\n",
    "    # We'd like to highlight those entries where the differences > delta_threshold\n",
    "    def highlight(x):\n",
    "        return ['background: yellow' if v > delta_threshold else '' for v in x]\n",
    "\n",
    "    def bold(x):\n",
    "        return ['font-weight: bold' if v > delta_threshold else '' for v in x]\n",
    "\n",
    "    # Highlith the entries\n",
    "    a = pd_merge.style.apply(highlight, subset=\"delta_percentage\").apply(\n",
    "        bold, subset=\"delta_percentage\")\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_unique_values(df):\n",
    "    \"\"\"Get unique values.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    df : pandas dataframe\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    unique : set\n",
    "    \"\"\"\n",
    "\n",
    "    unique = set([i[0] for i in df.dropna().values])\n",
    "    return unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "def get_features_correlation(feature1, feature2, df):\n",
    "    \"\"\"Get feature correlation.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature1 : string\n",
    "        Feature No1's name\n",
    "    feature2 : string\n",
    "        Feature No2's name\n",
    "    df : pandas dataframe\n",
    "        Generally either the train or the test set\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Dropping NaN\n",
    "    #print(\"Shape BEFORE removing NaN: \", df.shape)    \n",
    "    #df = df.dropna(how = 'any', axis = 0)\n",
    "    #print(\"Shape AFTER removing NaN: \", df.shape)\n",
    "    \n",
    "    # Getting rid of nan otherwise it will not work!\n",
    "    data1 = df[feature1]\n",
    "    data2 = df[feature2]\n",
    "\n",
    "    data1 = data1.values\n",
    "    data2 = data2.values\n",
    "\n",
    "    print(\"Are \", feature1, \" and \", feature2, \" correlated?\")\n",
    "    # Calculate spearman's correlation\n",
    "    coef, p = spearmanr(data1, data2)\n",
    "    print('Spearmans correlation coefficient: %.3f' % coef)\n",
    "    # Interpret the significance\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('Samples are NOT correlated (fail to reject H0) p=%.3f' % p)\n",
    "    else:\n",
    "        print('Samples are correlated (reject H0) p=%.3f' % p)\n",
    "\n",
    "    # Calculate kendall's correlation\n",
    "    coef, p = kendalltau(data1, data2)\n",
    "    print(\"-------------------------\")\n",
    "    print('Kendall correlation coefficient: %.3f' % coef)\n",
    "    # Interpret the significance given a value of alpha\n",
    "    alpha = 0.05\n",
    "    if p > alpha:\n",
    "        print('Samples are NOT correlated (fail to reject H0) p=%.3f' % p)\n",
    "    else:\n",
    "        print('Samples are correlated (reject H0) p=%.3f' % p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_non_common_values(df1, df2):\n",
    "    \"\"\"Get non common values.\n",
    "\n",
    "    Given two dataframes, df1 and df2, find the values\n",
    "    that are NOT common. This is the union of values that\n",
    "    are in df1 but not in df2 and viceversa.\n",
    "    \n",
    "    Paramaters\n",
    "    ----------\n",
    "    df1 : pandas dataframe\n",
    "    df2 : pandas dataframe\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    c : set\n",
    "        Set containing entries that are NOT common\n",
    "    \"\"\"\n",
    "    \n",
    "    # Get values that are in df1 but not in df2\n",
    "    a = set(get_unique_values(df1).difference(get_unique_values(df2)))\n",
    "    # Get values that are in df2 but not in df1\n",
    "    b = set(get_unique_values(df2).difference(get_unique_values(df1)))\n",
    "    c = a.difference(b)\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def plot_box_stripplot_violin_plots(df, features, y):\n",
    "    \"\"\"Plot how the target varies wrt feature changes.\n",
    "\n",
    "    This is achieved via three plots:\n",
    "    [1] box plot :\n",
    "    [2] stro plot :\n",
    "    [3] violin plot : \n",
    "    [4] bar plot :  A bar plot represents an estimate of central \n",
    "    tendency for a numeric variable with the height of each rectangle\n",
    "    and provides some indication of the uncertainty around that \n",
    "    estimate using error bars.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    features : list of string\n",
    "        List of features name\n",
    "\n",
    "    y : string\n",
    "        The target variable name\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    for i in features:\n",
    "        rcParams['font.size'] = 20\n",
    "        if i != \"SalePrice\":\n",
    "            fig, ax = plt.subplots(4, 1, figsize=(20, 15))\n",
    "            sns.boxplot(data=df, x=i, y=y, ax=ax[0])\n",
    "            sns.stripplot(data=df, x=i, y=y, ax=ax[1])\n",
    "            sns.violinplot(data=df, x=i, y=y, ax=ax[2])\n",
    "            sns.barplot(x=i, y=y, data=df, ax=ax[3])\n",
    "            plt.tight_layout()\n",
    "            plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_boxen_count_2x1(df, first_feature, figsize, target):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Create boxen plot of first_feature and Log_SalePrice\n",
    "    ax2 = plt.subplot(212)\n",
    "    sns.boxenplot(x=first_feature, y=target, data=df, color='tomato')\n",
    "    plt.xticks(rotation='horizontal')\n",
    "\n",
    "    # Create countplot of first_feature\n",
    "    ax1 = plt.subplot(211, sharex=ax2)\n",
    "    sns.countplot(x=first_feature, data=df, color='dimgrey')\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    plt.xlabel('')\n",
    "\n",
    "    # Adjusting the spaces between graphs\n",
    "    plt.subplots_adjust(hspace=0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def get_jointplot(df, feature, target):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    sns.jointplot(x=feature,\n",
    "                  y=target,\n",
    "                  data=train,\n",
    "                  kind='reg',\n",
    "                  height=9,\n",
    "                  color='darkseagreen')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def analyse_single_feature(feature_name, train, test, correlated_feature=\"None\", target=\"None\"):\n",
    "    \"\"\"Analyse single feature.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    feature_name : string\n",
    "        Name of the feature\n",
    "    train : pandas dataframe\n",
    "    test : pandas dataframe\n",
    "    correlated_featire : string, default \"None\"\n",
    "        The name of the feature we'd like to know is correlated\n",
    "        with feature_name\n",
    "    target : string, default \"None\"        \n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "\n",
    "    # Turn pandas series into pandas dataframe\n",
    "    current_feature = feature_name\n",
    "    current_train_feature_df = pd.DataFrame(train[current_feature])\n",
    "    current_test_feature_df = pd.DataFrame(test[current_feature])\n",
    "\n",
    "    print(\"\\nUnique TRAIN values\")\n",
    "    print(get_unique_values(current_train_feature_df))\n",
    "    print(\"\\nUnique TEST values\")\n",
    "    print(get_unique_values(current_test_feature_df))\n",
    "    print(\"\\nNon common values\")\n",
    "    print(\"Present in TRAIN but not in TEST:\", get_non_common_values(\n",
    "        current_train_feature_df, current_test_feature_df))\n",
    "    if len(get_non_common_values(current_train_feature_df, current_test_feature_df)) != 0.0:\n",
    "        \"\"\"\n",
    "        Print the warning ONLY for categorical features\n",
    "        In theory you can also do for numerical values, but this needs to be done of the value are outside\n",
    "        the min and max in my opinion\n",
    "        \"\"\"\n",
    "        if all(current_train_feature_df.dtypes == object) and all(current_test_feature_df.dtypes == object):\n",
    "            print(\"WARNING!\")\n",
    "            print(\"TEST set is not representive of the TRAIN set. Deal with it.\")\n",
    "            print(\"WARNING!\")\n",
    "\n",
    "    print(\"Present in TEST  but not in TRAIN\", get_non_common_values(\n",
    "        current_test_feature_df, current_train_feature_df))\n",
    "    if len(get_non_common_values(current_test_feature_df, current_train_feature_df)) != 0.0:\n",
    "        \"\"\"\n",
    "        Print the warning ONLY for categorical features\n",
    "        In theory you can also do for numerical values, but this needs to be done of the value are outside\n",
    "        the min and max in my opinion\n",
    "        \"\"\"\n",
    "        if all(current_train_feature_df.dtypes == object) and all(current_test_feature_df.dtypes == object):\n",
    "            print(\"WARNING!\")\n",
    "            print(\"This is serious!\")\n",
    "            print(\"WARNING!\")\n",
    "\n",
    "    print(\"\\nNon-usable entries\")\n",
    "    dsp(compare_sets_over_non_usable_entries(\n",
    "        current_train_feature_df, current_test_feature_df))\n",
    "\n",
    "    print(\"\\nBasics details TRAIN\")\n",
    "    dsp(basic_details(current_train_feature_df))\n",
    "    dsp(get_IQR_frequency(current_train_feature_df))\n",
    "\n",
    "    print(\"\\nBasics details TEST\")\n",
    "    dsp(basic_details(current_test_feature_df))\n",
    "    dsp(get_IQR_frequency(current_test_feature_df))\n",
    "\n",
    "    print(\"\\nCompare TRAIN and TEST histogram and kde\")\n",
    "    compare_hist_kde(current_train_feature_df, current_test_feature_df)\n",
    "\n",
    "    if target != \"None\":\n",
    "        # Only for the train set as the test set does not have a target column\n",
    "        create_boxen_count_2x1(train, feature_name, (20, 8), target)\n",
    "        plot_box_stripplot_violin_plots(train, [feature_name], target)\n",
    "\n",
    "    # Only for numerical features\n",
    "    if all(current_train_feature_df.dtypes != object) and all(current_test_feature_df.dtypes != object):\n",
    "\n",
    "        print(\"\\nNo of value with equal to zero.\")\n",
    "        \"\"\"\n",
    "        This important when using a log transform\n",
    "        \"\"\"\n",
    "        print(\"No of entries equal to 0.0 in TRAIN set\", list(\n",
    "            current_train_feature_df.values).count(0.0))\n",
    "        print(\"No of entries equal to 0.0 in TEST set\", list(\n",
    "            current_test_feature_df.values).count(0.0))\n",
    "\n",
    "        print(\"\\nCompare TRAIN and TEST distributions\")\n",
    "        dsp(compare_distribution_sets_on_numerical_columns(\n",
    "            current_train_feature_df, current_test_feature_df))\n",
    "\n",
    "        print(\"\\nNormality on the TRAIN set\")\n",
    "        dsp(test_set_for_normality(current_train_feature_df))\n",
    "\n",
    "        print(\"\\nNormality on the TEST set\")\n",
    "        dsp(test_set_for_normality(current_test_feature_df))\n",
    "\n",
    "        # Scatter plot for target vs. feature\n",
    "        if target != \"None\":\n",
    "            print(\"Scatter plot for target vs. feature\")\n",
    "            #data = pd.concat([train[target], current_train_feature_df], axis=1)\n",
    "            #data.plot.scatter(x=current_feature, y=target)\n",
    "\n",
    "            fig, (ax1, ax2) = plt.subplots(\n",
    "                figsize=(16, 8), ncols=2, sharey=False)\n",
    "            # This is done if the realtionship is linear or not!\n",
    "            ax1.title.set_text('Scatter + regression plor + variance')\n",
    "            sns.regplot(x=train[feature_name], y=train[target], ax=ax1)\n",
    "            # This is the residual plot which shows the error variance\n",
    "            ax2.title.set_text('Regression plot errors')\n",
    "            sns.residplot(train[feature_name], train[target], ax=ax2)\n",
    "\n",
    "            get_jointplot(train, feature_name, target)\n",
    "\n",
    "        # dropna is necessary otherwise it'd not run\n",
    "        # the try-except is there in case it is a categorical variable\n",
    "        print(\"QQ plot on TRAIN set for original and normalised values\")\n",
    "        compare_QQ_kde(train[current_feature].dropna(),\n",
    "                       np.log1p(train[current_feature].dropna()))\n",
    "\n",
    "        print(\"QQ plot on TEST set for original and normalised values\")\n",
    "        compare_QQ_kde(test[current_feature].dropna(),\n",
    "                       np.log1p(test[current_feature].dropna()))\n",
    "\n",
    "        if correlated_feature != \"None\":\n",
    "            print(\"Correlation against \", correlated_feature,\n",
    "                  \" on the train set ONLY!\")\n",
    "            get_features_correlation(feature_name, correlated_feature, train)\n",
    "\n",
    "    else:\n",
    "        print(current_feature, \" is NOT numerical!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature** - MSSubClass Identifies the type of dwelling involved in the sale.\t\n",
    "\n",
    "        20\t1-STORY 1946 & NEWER ALL STYLES\n",
    "        30\t1-STORY 1945 & OLDER\n",
    "        40\t1-STORY W/FINISHED ATTIC ALL AGES\n",
    "        45\t1-1/2 STORY - UNFINISHED ALL AGES\n",
    "        50\t1-1/2 STORY FINISHED ALL AGES\n",
    "        60\t2-STORY 1946 & NEWER\n",
    "        70\t2-STORY 1945 & OLDER\n",
    "        75\t2-1/2 STORY ALL AGES\n",
    "        80\tSPLIT OR MULTI-LEVEL\n",
    "        85\tSPLIT FOYER\n",
    "        90\tDUPLEX - ALL STYLES AND AGES\n",
    "       120\t1-STORY PUD (Planned Unit Development) - 1946 & NEWER\n",
    "       150\t1-1/2 STORY PUD - ALL AGES\n",
    "       160\t2-STORY PUD - 1946 & NEWER\n",
    "       180\tPUD - MULTILEVEL - INCL SPLIT LEV/FOYER\n",
    "       190\t2 FAMILY CONVERSION - ALL STYLES AND AGES\n",
    "- **Missing value**: NONE\n",
    "- **Type**: Numeric-> treated as categorical -> use one-hot encoding.\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments:** Which one between ordinal or label encoder should we use? Now let us separate the specifics of a package implementation and let us focus on the high level understand of the 3 options:\n",
    "    - **Ordinal encoding** should be used for ordinal variables (where order matters, like cold, warm, hot);\n",
    "    - **Label encoding** should be used for non-ordinal (aka nominal) variables (where order doesn't matter, like blonde, brunette). No extra column is added but the algorithm may used the value as if they were ordinal.\n",
    "    - **One-hot encoding** every entry get a 1, but the number of column is equal to the number of entries minus one\n",
    "- A note on the implementation: in the sklearn implementation, if you want ordinal encoding (order is preserved); you must do the ordinal encoding yourself (neither OrdinalEncoder nor LabelEncoder can infer the order! See the OrdinalEncoder constructor parameter called `categories`).\n",
    "- **EDA**: Each of these classes represents a very different style of building, as shown in the data description. Hence, we can see large variance between classes with SalePrice. After turning the entris from numerical to string I will use one-hot encoding because I am unable to establish an order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"MSSubClass\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: MSZoning Identifies the general zoning classification of the sale.\n",
    "\t\t\n",
    "       A\tAgriculture\n",
    "       C\tCommercial\n",
    "       FV\tFloating Village Residential\n",
    "       I\tIndustrial\n",
    "       RH\tResidential High Density\n",
    "       RL\tResidential Low Density\n",
    "       RP\tResidential Low Density Park \n",
    "       RM\tResidential Medium Density\n",
    "\t\n",
    "    \n",
    "- **Missing value**: 4\n",
    "- **Cardinality**: LOW\n",
    "- **Type**: Ordinal \n",
    "- **Encoding needed**: Yes -> Use one-hot encoding\n",
    "- **Comments:**: We could make some assumptions on what type of areas are worth more and try to come up with a classification, but I feel this will mean enforcing a personal bias on the encoding. Therefore, I will use one-hot encoding for this feature. Also note that, not all the entries mentioned in the description are present. For instance there are no Industrial zones in the dataset.\n",
    "- **EDA:** Most of the houses are located in RL (residential low population density area) and Houses located at lower population density area generally have higher SalePrice than houses at higher population density area. Nevertheless, this is not enough to impose an order in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"MSZoning\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: LotFrontage: Linear feet of street connected to property\n",
    "- **Missing value**: high above 15% for each train and test sets\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments** Maybe we could group the data into different categories (binning)?\n",
    "- **EDA** There seems to be some outliers having very high LotFrontages values but relatively low SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"LotFrontage\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: LotArea: Lot size in square feet\n",
    "- **Missing value**: No\n",
    "- **Cardinality**: High\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments** Maybe grouped into different bins? We'll not use this in this notebook, but I'll share how to do it under the `feature_engineering` section.\n",
    "- **EDA** This feature shows a high correlation but it is very positively skewed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"LotArea\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Street Type of road access to property\n",
    "- **Missing value**: NONE\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Ordinal categorical, as having a paved street is better than having a gravel one!\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments:** Similarly, one can also argue that there is not enough data for the \"Grvl\"  making this feature useless. Another, option would be to create a isPaved feature which essentially will create two bin, has it and hasn't it! What I'd like to use here is ordinal encoding as having a paved street type seems to lead to higher sale price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Street\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Alley: Type of alley access to property\n",
    "- **Missing value**: over 90%\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> use ordinal endoder\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Use NA for missing value imputation\n",
    "- **EDA**: Here we see a fairly even split between to two classes in terms of frequency, but \n",
    "a much higher average SalePrice for Paved alleys as opposed to Gravel ones. Having a paved Alley\n",
    "seems to be correlated with higher prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Alley\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: LotShape: General shape of property\n",
    "       Reg\tRegular\t\n",
    "       IR1\tSlightly irregular\n",
    "       IR2\tModerately Irregular\n",
    "       IR3\tIrregular        \n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> Use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments:** I was not able to clearly see a consistent trend from regular to irregular, hence I am not going to use ordinal encoding. Another option could be to add another feature flagging regular or irregular shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"LotShape\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: LandContour: Flatness of the property\n",
    "       Lvl\tNear Flat/Level\t\n",
    "       Bnk\tBanked - Quick and significant rise from street grade to building\n",
    "       HLS\tHillside - Significant slope from side to side\n",
    "       Low\tDepression        \n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Since this a categorical feature without order, I will create dummy features. Note that most properties are levelled; thus we could add another category feature as in \"not_level\".\n",
    "- **EDA**: Most houses are indeed on a flat contour, however the houses with the highest SalePrice seem to come from properties on a hill! Better view!?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"LandContour\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Utilities: Type of utilities available\n",
    "\t\t\n",
    "       AllPub\tAll public Utilities (E,G,W,& S)\t\n",
    "       NoSewr\tElectricity, Gas, and Water (Septic Tank)\n",
    "       NoSeWa\tElectricity and Gas Only\n",
    "       ELO\tElectricity only\t\n",
    "- **Missing value**: 2\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: The vast majority of entries have AllPub and it is reasonable to assume that those 2 value missing are essentially likely to be AllPub. Further if this is the case there is essentially no variability in the test data. It is a constant. We have seen how `NoSeWa` is only present in the train data, but not in the test set we have access to. To complicate the situation we have only one entry. This means that:\n",
    "    - If we drop this feature and the private set has one value with `NoSeWa` we then have a model that was not trained for this.\n",
    "    - Even if we'd like to train a model we litteraly have only one entry. How can we train and test this? We could try to collect more data but this is not an option available to us.\n",
    "    - Droping this feature seems to be an almost forced option. However, consider that selecting only the top feature would generally do this for us, probably we do not even need to drop it.\n",
    "    - A more elaborated feature would to consider a flag hasAllPub with Yes and No and the hot-encode that feature. I feel this is the most robust option we have. Again we can still rely on feature importance to exclude this option is deemed not useuful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Utilities\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: LotConfig: Lot configuration\n",
    "\n",
    "       Inside\tInside lot\n",
    "       Corner\tCorner lot\n",
    "       CulDSac\tCul-de-sac\n",
    "       FR2\tFrontage on 2 sides of property\n",
    "       FR3\tFrontage on 3 sides of property\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Ordinal -> use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **EDA**: Cul de sac's seems to boast the highest average prices within Ames, however most houses are positioned inside or on the corner of the lot. To simplify this feature we could cluster \"FR2\" and \"FR3\", then create dummy features afterwards. This will have the benefit to limit the number of extra column created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"LotConfig\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: LandSlope: Slope of property\n",
    "\t\t\n",
    "       Gtl\tGentle slope\n",
    "       Mod\tModerate Slope\t\n",
    "       Sev\tSevere Slope        \n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Ordinal categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: We are going to cluster \"Mod\" and \"Sev\" to create one class, and create a new flag to indicate a gentle slope or not. This will also help us reduce the number of column while one-hot encoding.\n",
    "- **EDA**:  Most houses have a gentle slope of land and overall, the severity of the slope doesn't appear to have much of an impact on SalePrice.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"LandSlope\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Neighborhood: Physical locations within Ames city limits\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical, but we do not have enouh info at the moment to rank them.\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Since this is a categorical feature without order, I will create dummy features.\n",
    "- **EDA**: Neighborhood clearly has an important contribution towards SalePrice, since we see such high values for certain areas and low values for others. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Neighborhood\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Condition1: Proximity to various conditions\n",
    "       Artery\tAdjacent to arterial street\n",
    "       Feedr\tAdjacent to feeder street\t\n",
    "       Norm\tNormal\t\n",
    "       RRNn\tWithin 200' of North-South Railroad\n",
    "       RRAn\tAdjacent to North-South Railroad\n",
    "       PosN\tNear positive off-site feature--park, greenbelt, etc.\n",
    "       PosA\tAdjacent to postive off-site feature\n",
    "       RRNe\tWithin 200' of East-West Railroad\n",
    "       RRAe\tAdjacent to East-West Railroad        \n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Oridnal -> Use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **Com ments**: Most of the houses have normal condition and there seems to be no clear order in the label. There is another feature called `Condition1` which seems very similar. Please take a look to how we dealt with this under the `feature_engineering` section.\n",
    "- **EDA**: Since this feature is based around local features, it is understandable that having more desirable things, like a parks... nearby are a factor that would contribute towards a higher SalePrice. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Condition1\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Condition2: Proximity to various conditions (if more than one is present)\n",
    "       Artery\tAdjacent to arterial street\n",
    "       Feedr\tAdjacent to feeder street\t\n",
    "       Norm\tNormal\t\n",
    "       RRNn\tWithin 200' of North-South Railroad\n",
    "       RRAn\tAdjacent to North-South Railroad\n",
    "       PosN\tNear positive off-site feature--park, greenbelt, etc.\n",
    "       PosA\tAdjacent to postive off-site feature\n",
    "       RRNe\tWithin 200' of East-West Railroad\n",
    "       RRAe\tAdjacent to East-West Railroad\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Most of the houses have normal condition and there seems to be no correlation with the target. Most of the houses have normal condition and there seems to be no clear order in the label. There is another feature called `Condition1` which seems very similar. Please take a look to how we dealt with this under the `feature_engineering` section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Condition2\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: BldgType: Type of dwelling\n",
    "       1Fam\tSingle-family Detached\t\n",
    "       2FmCon\tTwo-family Conversion; originally built as one-family dwelling\n",
    "       Duplx\tDuplex\n",
    "       TwnhsE\tTownhouse End Unit\n",
    "       TwnhsI\tTownhouse Inside Unit        \n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: The different categories exhibit a range of average SalePrice. The class with the most observations is \"1Fam\". We can also see that the variance within classes is quite tight, with only a few extreme values in each case. There could be a possibility to cluster these classes, however for now I am going to create dummy features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"BldgType\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: HouseStyle: Style of dwelling\n",
    "       1Story\tOne story\n",
    "       1.5Fin\tOne and one-half story: 2nd level finished\n",
    "       1.5Unf\tOne and one-half story: 2nd level unfinished\n",
    "       2Story\tTwo story\n",
    "       2.5Fin\tTwo and one-half story: 2nd level finished\n",
    "       2.5Unf\tTwo and one-half story: 2nd level unfinished\n",
    "       SFoyer\tSplit Foyer\n",
    "       SLvl\tSplit Level        \n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes -> see the `feature_engineering` section to see how we dealt with it\n",
    "- **Comments**: Here we see quite a few extreme values across the categories and a large weighting of observations towards the integer story houses. Although the highest average SalePrice comes from \"2.5Fin\", this has a very high standard deviation and therefore more reliably, the \"2Story\" houses are also very highly priced on average. Since there are some categories with very few values, I will cluster these into another category and create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"HouseStyle\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: OverallQual: Rates the overall material and finish of the house\n",
    "\n",
    "       10\tVery Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\n",
    "       5\tAverage\n",
    "       4\tBelow Average\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor        \n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Numerical but is really categorical with an precise order\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: Others have pointed out there is one outliers at OveralQual ~ 5 with a SalePrice which istoo high. We'll check later if our automated outliers procedure has removed it or not. This feature although being numeric is actually categoric and ordinal. Oridnal because as the value increases so does the SalePrice. We see here a nice positive correlation with the increase in OverallQual and the SalePrice, as you'd expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"OverallQual\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: OverallCond: Rates the overall condition of the house\n",
    "       10\tVery Excellent\n",
    "       9\tExcellent\n",
    "       8\tVery Good\n",
    "       7\tGood\n",
    "       6\tAbove Average\t\n",
    "       5\tAverage\n",
    "       4\tBelow Average\t\n",
    "       3\tFair\n",
    "       2\tPoor\n",
    "       1\tVery Poor\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: Interestingly, we see here that it does follow a positive correlation with SalePrice, however we see a peak at a value of 5, along with a high number of observations at this value. The highest average SalePrice actually comes from a value of 5 as opposed to 10. For this feature, I will leave it as being numeric and ordinal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"OverallCond\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We are here exploring how much features `OverallQual` and `OverallCond` are correlated to each others.\n",
    "    - Option #1 - leave them as they are if we know they are correlated (bad option)\n",
    "    - Option #2 - get the mean between them and drop the two feature (better option)\n",
    "    - Option #3 - get the mean and keep the two features, three in total (worst option)\n",
    "    - Option #4 - keep only one feature (best option)\n",
    "- What follows is an attempt to study the relationship between these two features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def compare_categorical_variables(feature1, feature2, df):\n",
    "    \"\"\"Compare two categorical variables.\n",
    "    \n",
    "    Highlghts entries that are not the same. This allows to make\n",
    "    a line-by-line comparison of the entries.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    feature1 : string\n",
    "        First feature's name \n",
    "    feature2 : string\n",
    "        Second feature's name\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pandas dataframe\n",
    "        This is where the line-by-line comparison is returned\n",
    "    \"\"\"\n",
    "    \n",
    "    df1 = df[feature1]\n",
    "    df2 = df[feature2]    \n",
    "    \n",
    "    result = []\n",
    "    for i, value in enumerate(df1.values):            \n",
    "        if df1.iloc[i] != df2.iloc[i]:\n",
    "            result.append(\"different\")\n",
    "        else:\n",
    "            result.append(\"equal\")\n",
    "    \n",
    "    # Create a pandas dataframe\n",
    "    dummy = pd.DataFrame()\n",
    "    dummy[feature1] = df1\n",
    "    dummy[feature2] = df2\n",
    "    dummy[\"Equal?\"] =  result\n",
    "    \n",
    "\n",
    "    # Decorate the dataframe for quick visualisation\n",
    "    def highlight(x):    \n",
    "        return ['background: yellow' if v == \"different\" else '' for v in x]\n",
    "\n",
    "    def bold(x):\n",
    "        return ['font-weight: bold' if v == \"different\" else '' for v in x]\n",
    "\n",
    "    # Visualise the highlighted df\n",
    "    return dummy.style.apply(highlight).apply(bold)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "compare_categorical_variables(\"OverallQual\", \"OverallCond\", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "compare_categorical_variables(\"OverallQual\", \"OverallCond\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The two dataframe above show most of the value are different in the term of absolute values, but that does not mean they are not correlated. In fact, if you look carefully even when the value are different, most of them capture the trend meaning: when the `OverallQual` is high then the `OverallCond` is also high.\n",
    "- Further, we can also use the Spearmans and Kendal's rank correlation to confirm if these two variables are correlated to each other.\n",
    "- As you can see both correlation tests suggest the two variables are stronlgy correlated. If that is the case, if we drop one variable, the piece of information dropped in then recovered by the presence of the other variable.\n",
    "- This makes me think that creating a `meanQuality` is not going to be such a great idea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features_correlation(\"OverallQual\", \"OverallCond\", train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_features_correlation(\"OverallQual\", \"OverallCond\", test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: YearBuilt: Original construction date\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Numerical -> treated as categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: It depends on whether it is used as a measurement of time (quantitative) or a name of a particular interval of time (categorical). It could be either. One of the test one can use to understand this is to ask whether the ratio between two years is meaningful or not. Taking the ratio betwteen two ueast is not meaningful which is why its not appropriate to classify it as a quantitative variable. However, one-hot encoding on all of it would be problematic. What I'd suggest would be to use a 7 to 10 year bin size window. This will lower the number of extra column being added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"YearBuilt\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Since we have suggested to use binning, we'd like to see how these would look like on both sets.\n",
    "- Notice how the bins differ; a confirmation of the differences between the two datasets.\n",
    "- It also shown how the bins would have looked if we had merged the two sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy = pd.cut(train['YearBuilt'], 7)\n",
    "for i in dummy.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy = pd.cut(test['YearBuilt'], 7)\n",
    "for i in dummy.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = pd.concat((train, test)).reset_index(drop=True)\n",
    "dummy = pd.cut(all_data['YearBuilt'], 7)\n",
    "for i in dummy.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: YearRemodAdd: Remodel date (same as construction date if no remodeling or additions)\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Numerical -> treated as categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: The newer the remodelling of a house, the higher the SalePrice. From the data description, I believe that creating a new feature describing the difference in number of years between remodeling and construction could be a good choice.\n",
    "- **EDA** This means that if we take the difference we can find out if the house ws recently remodelled. This could be used as an extra feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"YearRemodAdd\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The number of years after the house was last refurbished **does not seem** to correlate well with the `SalePrice`. \n",
    "- I guess this will correlated well with the delta price if we had only the previous sale price. Since we do not have it, I will suggest we drop this feature `YearRemodAdd`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy = train['YearRemodAdd'] - train['YearBuilt']\n",
    "\n",
    "plt.subplots(figsize=(40, 10))\n",
    "sns.barplot(x=dummy, y=train[\"SalePrice\"])\n",
    "plt.xticks(rotation=90);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: RoofStyle: Type of roof\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: This feature has two highly frequent categories. Since this is a categorical feature without order, I will create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"RoofStyle\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: RoofMatl: Roof material\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Interestingly, there are very few observations in the training data for several classes. \n",
    "- **EDA**: The two sets dot not have the same entries. This will cause some issues while encoding the feature if we treat the train and set sets separately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"RoofMatl\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Exterior1st: Exterior covering on house\n",
    "\n",
    "       AsbShng\tAsbestos Shingles\n",
    "       AsphShn\tAsphalt Shingles\n",
    "       BrkComm\tBrick Common\n",
    "       BrkFace\tBrick Face\n",
    "       CBlock\tCinder Block\n",
    "       CemntBd\tCement Board\n",
    "       HdBoard\tHard Board\n",
    "       ImStucc\tImitation Stucco\n",
    "       MetalSd\tMetal Siding\n",
    "       Other\tOther\n",
    "       Plywood\tPlywood\n",
    "       PreCast\tPreCast\t\n",
    "       Stone\tStone\n",
    "       Stucco\tStucco\n",
    "       VinylSd\tVinyl Siding\n",
    "       Wd Sdng\tWood Siding\n",
    "       WdShing\tWood Shingles        \n",
    "- **Missing value**: 1 in the test set\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> see `Feature_engineering`\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: \n",
    "- **EDA**: Looking at these 2 features (`Exterior1st` and `Exterior2nd`) together, we can see that they exhibit very similar behaviours against SalePrice. This tells me that they are very closely related. Hence, I will create a flag to indicate whether there is a different 2nd exterior covering to the first. Then I will keep \"Exterior1st\" and create dummy variables from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Exterior1st\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Exterior2nd: Exterior covering on house (if more than one material)\n",
    "- **Missing value**: 1 in the test set\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> see `Feature_engineering`\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: use the mode for the missing value imputation.\n",
    "- **EDA**: see the discussion for `Exterior1st`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Exterior2nd\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: MasVnrType: Masonry veneer type\n",
    "\n",
    "       BrkCmn\tBrick Common\n",
    "       BrkFace\tBrick Face\n",
    "       CBlock\tCinder Block\n",
    "       None\tNone\n",
    "       Stone\tStone\n",
    "- **Missing value**: ~ 20\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: \n",
    "- **EDA**: Each class has quite an unique range of values for `SalePrice`. The only class that stands out is `BrkCmn` which has a low frequency. Clearly `Stone` demands the highest `SalePrice` on average, although there are some extreme values within `BrkFace`. Since this is a categorical feature without order, I will create dummy variables here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"MasVnrType\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: MasVnrArea: Masonry veneer area in square feet\n",
    "- **Missing value**: ~ 20\n",
    "- **Cardinality**: 22%\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: The median is zero, this means that 50% of the entries have zero `MasVnrArea`. Using the 75% percentile would give an estimate that is very much biased toward the highest value. The mean seems a good compromise.\n",
    "- **EDA**: This feature has negligible correlation with `SalePrice`, and the values for this feature vary widely based on house type, style and size. Since this feature is insignificant in regards to SalePrice, and it also correlates highly with `MasVnrType` (if \"MasVnrType = \"None\" then it has to be equal to 0), I will drop this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"MasVnrArea\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: ExterQual, Evaluates the quality of the material on the exterior \n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       Po\tPoor    \n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> ordinal\n",
    "- **Encoding needed**: Yes\n",
    "- **EDA**: This feature shows a clear order and has a positive correlation with `SalePrice`. As the quality increases, so does the `SalePrice`. We see the largest number of observations within the two middle classes, and the lowest observations within the lowest class. Since this is a categorical feature with order, I will replace these values by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"ExterQual\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: ExterCond, Evaluates the present condition of the material on the exterior\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       Po\tPoor    \n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **EDA**: Interestingly we see the largest values of SalePrice for the second and third best classes. This is perhaps because of the large frequency of values within these classes, whereas we only see 3 observations within \"Ex\" from the training data. Since this categorical feature has an order, **but the SalePrice does not necessarily correlate with this order, I will create dummy variables**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"ExterCond\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Type of foundation\n",
    "       BrkTil\tBrick & Tile\n",
    "       CBlock\tCinder Block\n",
    "       PConc\tPoured Contrete\t\n",
    "       Slab\tSlab\n",
    "       Stone\tStone\n",
    "       Wood\tWood\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Creating some ordinal label will not make sense here.\n",
    "- **EDA**: We have 3 classes with high frequency but we also have 3 of low frequency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Foundation\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: BsmtQual: Evaluates the height of the basement\n",
    "       Ex\tExcellent (100+ inches)\t\n",
    "       Gd\tGood (90-99 inches)\n",
    "       TA\tTypical (80-89 inches)\n",
    "       Fa\tFair (70-79 inches)\n",
    "       Po\tPoor (<70 inches\n",
    "       NA\tNo Basement\n",
    "- **Missing value**: ~80\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: The description tells us that nan is probably going to be a NA -> no basement.\n",
    "- **EDA**: `SalePrice` is clearly affected by `BsmtQual`: the better the quality the higher the price. However, it looks as though most houses have either `Good` or `Typical` sized basements. Since this feature is ordinal, i.e. the categories represent different levels of order, I will replace the values by hand. Also note that there is no `Po` entries available neither in the train nor test set. I've seen from other kagglers that people tend not to encode this value if there are no entries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"BsmtQual\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: BsmtCond: Evaluates the general condition of the basement\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical - slight dampness allowed\n",
    "       Fa\tFair - dampness or some cracking or settling\n",
    "       Po\tPoor - Severe cracking, settling, or wetness\n",
    "       NA\tNo Basement\n",
    "- **Missing value**: ~80\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments** The description tells us that nan is probably going to be a NA -> not basement.\n",
    "- **EDA**: As the condition of the basement improves, the SalePrice also increases. However, we see some very high SalePrice values for the houses with \"Typical\" basement conditions. This perhaps suggests that although these two features correlate positively, BsmtCond may not have a largely influential contribution on SalePrice. We also see the largest number of houses falling into the \"TA\" category. Since this feature is ordinal, I will replace the values by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"BsmtCond\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- We'll also note that having no basement correlates better than having one but in poor in general condition! \n",
    "- However, when compared to `BsmtQual` the trend is not replicated, meaning that not having a basement shows less correlation than having one in fair condition. \n",
    "- Unfortunately, this statement is very weak because we have no entries with `Po` in for the `BsmtQual` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.subplots(figsize=(20, 7))\n",
    "\n",
    "dummy = copy.deepcopy(train)\n",
    "dummy['BsmtCond'] = dummy['BsmtCond'].fillna(\"NA\")\n",
    "dummy['BsmtQual'] = dummy['BsmtQual'].fillna(\"NA\")\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "sns.boxplot(x=\"BsmtCond\", y=\"SalePrice\", data=dummy,\n",
    "            order=['NA', 'Po', 'Fa', 'TA', 'Gd'])\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "sns.boxplot(x=\"BsmtQual\", y=\"SalePrice\", data=dummy,\n",
    "            order=['NA', 'Po', 'Fa', 'TA', 'Gd'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: BsmtExposure: Refers to walkout or garden level walls\n",
    "       Gd\tGood Exposure\n",
    "       Av\tAverage Exposure (split levels or foyers typically score average or above)\t\n",
    "       Mn\tMimimum Exposure\n",
    "       No\tNo Exposure\n",
    "       NA\tNo Basement\n",
    "- **Missing value**: ~80\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> ordinal\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Use `No` Basement as null values imputation\n",
    "- **EDA**: As the amount of exposure increases, so does the typical `SalePrice`. Interestingly, the average difference of `SalePrice` between categories is quite low here which is telling me that some houses are sold for very high prices, even with no exposure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"BsmtExposure\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: BsmtFinType1\n",
    "       GLQ\tGood Living Quarters\n",
    "       ALQ\tAverage Living Quarters\n",
    "       BLQ\tBelow Average Living Quarters\t\n",
    "       Rec\tAverage Rec Room\n",
    "       LwQ\tLow Quality\n",
    "       Unf\tUnfinshed\n",
    "       NA\tNo Basement    \n",
    "- **Missing value**: ~79\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Use NA Basement as null value imputation\n",
    "- **EDA**: Houses with an unfinished basement on average sold for more money than houses having an average rating. However, houses with a good finish within the basement still demand more money than unfinished ones. This is an ordinal feature, however as you can see this order does not necessarily cause a higher SalePrice. **By creating an ordinal variable we are suggesting that as the order of the feature increases, then the target variable would increase as well. We can see that this is not the case. Therefore, I will create dummy variables from this feature.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"BsmtFinType1\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: BsmtFinSF1: Type 1 finished square feet\n",
    "- **Missing value**: ~1\n",
    "- **Cardinality**: ~ 40%\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: No (yes if we use bins!)\n",
    "- **Comments**: Use 0.0 as null value imputation. The mean will be too skewed to the right due to outliers.\n",
    "- **EDA**: This feature has a positive correlation with SalePrice and the spread of data points is quite large. It is also clear that the local area (Neighborhood) and style of building (BldgType, HouseStyle and LotShape) has a varying effect on this feature. Since this is a continuous numeric feature we have two options:\n",
    "    - Leave it as it is \n",
    "    - Bin it and then use one-hot encoding. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"BsmtFinSF1\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_train = pd.cut(train['BsmtFinSF1'], 4)\n",
    "for i in dummy_train.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_test = pd.cut(test['BsmtFinSF1'], 4)\n",
    "for i in dummy_test.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: BsmtFinType2: Rating of basement finished area (if multiple types)\n",
    "       GLQ\tGood Living Quarters\n",
    "       ALQ\tAverage Living Quarters\n",
    "       BLQ\tBelow Average Living Quarters\t\n",
    "       Rec\tAverage Rec Room\n",
    "       LwQ\tLow Quality\n",
    "       Unf\tUnfinshed\n",
    "       NA\tNo Basement\n",
    "- **Missing value**: ~80\n",
    "- **Cardinality**: ~ Low\n",
    "- **Type**: Categorical -> use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Use Na Basement as null value imputation\n",
    "- **EDA**: There are a lot of houses with unfinished second basements, and this may cause the skew in terms of SalePrice's being relatively high for these. There are only a few values for each of the other categories, with the highest average SalePrice coming from the second best category. Although this is intended to be an ordinal feature, we can see that the SalePrice does not necessarily increase with order. Thus, I will use one-hot encoding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"BsmtFinType2\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: BsmtFinSF2, Type 2 finished square feet\n",
    "       GLQ\tGood Living Quarters\n",
    "       ALQ\tAverage Living Quarters\n",
    "       BLQ\tBelow Average Living Quarters\t\n",
    "       Rec\tAverage Rec Room\n",
    "       LwQ\tLow Quality\n",
    "       Unf\tUnfinshed\n",
    "       NA\tNo Basement    \n",
    "- **Missing value**: 1\n",
    "- **Cardinality**: ~ 10%\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: \n",
    "- **EDA**: There are a large number of data points with this feature = 0. Apart from this, there is no significant correlation with `SalePrice` and there is a large spread of values. Hence, I will replace this feature with a flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"BsmtFinSF2\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: BsmtUnfSF, Unfinished square feet of basement area\n",
    "- **Missing value**: 1\n",
    "- **Cardinality**: ~ 50%\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Use median for nan value imputation\n",
    "- **EDA**: This feature has a significant positive correlation with SalePrice, with a small proportion of data points having a value of 0. This tells me that most houses will have some amount of square feet unfinished within the basement, and this actually positively contributes towards SalePrice. The amount of unfinished square feet also varies widely based on location and style. Whereas the average unfinished square feet within the basement is fairly consistent across the different lot shapes. Since this is a continuous numeric feature with a significant correlation, one option could be to create bins and then create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"BsmtUnfSF\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_train = pd.cut(train['BsmtUnfSF'], 4)\n",
    "for i in dummy_train.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_test = pd.cut(test['BsmtUnfSF'], 4)\n",
    "for i in dummy_test.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: TotalBsmtSF, Total square feet of basement area\n",
    "- **Missing value**: 1\n",
    "- **Cardinality**: ~ 50%\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments** Use median for nan value imputation. More importantly is the knowledge that the distribution is NOT normal and the fact that that are many entries with zeros. If we were to use a log transform then we'll have to deal with those entries that have a zero value assigned to them. In the figure below we can see how the log transform on the original data would not work because of these zero entries. We'll take note of that and do something about it later one.\n",
    "- **EDA**: This is a very important feature within my analysis, due to such a high correlation with `Saleprice`. We can see that it varies widely based on location, however the average basement size has a lower variance based on type, style and lot shape. I will create some binnings and dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"TotalBsmtSF\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_train = pd.cut(train['TotalBsmtSF'], 10)\n",
    "for i in dummy_train.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_test = pd.cut(train['TotalBsmtSF'], 10)\n",
    "for i in dummy_test.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Heating\n",
    "       Floor\tFloor Furnace\n",
    "       GasA\tGas forced warm air furnace\n",
    "       GasW\tGas hot water or steam heat\n",
    "       Grav\tGravity furnace\t\n",
    "       OthW\tHot water or steam heat other than gas\n",
    "       Wall\tWall furnace\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Use median for nan value imputation\n",
    "- **EDA**: We see the highest frequency and highest average SalePrice coming from \"GasA\" and a very low frequency from all other classes. Hence, I will create a flag to indicate whether \"GasA\" is present or not and drop the Heating feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Heating\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: HeatingQC\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> ordinal\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Use median for nan value imputation\n",
    "- **EDA**: Here we see a positive correlation with SalePrice as the heating quality increases. With \"Ex\" bringing the highest average SalePrice. We also see a high number of houses with this heating quality too, which means most houses had very good heating! This is a categorical feature which exhibits an order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"HeatingQC\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: CentralAir\n",
    "       N\tNo\n",
    "       Y\tYes\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical -> ordinal\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Use median for nan value imputation\n",
    "- **EDA**: - We see that houses with central air conditioning are able to demand a higher average `SalePrice` than ones without. For this feature, I will simply replace the categories with numbers 0 and 1. However, please consider this:\n",
    "    - If we use 0, 1, we are also implicitly giving an order as 0 <1, which seems appropriate here.\n",
    "    - If we use one-hot encoding then the algorithm would not see an order and the risk of the algorithm learning a possible order would be eliminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"CentralAir\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Electrical system\n",
    "       SBrkr\tStandard Circuit Breakers & Romex\n",
    "       FuseA\tFuse Box over 60 AMP and all Romex wiring (Average)\t\n",
    "       FuseF\t60 AMP Fuse Box and mostly Romex wiring (Fair)\n",
    "       FuseP\t60 AMP Fuse Box and mostly knob & tube wiring (poor)\n",
    "       Mix\tMixed\n",
    "- **Missing value**: 1\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Use mode for nan value imputation\n",
    "- **EDA**:  We see the highest average `SalePrice` coming from houses with `SBrkr` electrics, and these are also the most frequent electrical systems installed in the houses from this area. We have 2 categories in particular that have very low frequencies, `FuseP` and `Mix`. I am going to cluster all the classes related to fuses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Electrical\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: 1stFlrSF: First Floor square feet\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: 50%\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: \n",
    "- **EDA**: Clearly this shows a very high positive correlation with SalePrice, this will be an important feature during modeling. Once again, this feature varies greatly across neighborhoods and the size of this feature varies across building types and styles. This feature does not vary so much across the lot size. Since this is a continuous numeric feature, once again I will bin this feature and create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"1stFlrSF\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_train = pd.cut(train['1stFlrSF'], 6)\n",
    "for i in dummy_train.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_test = pd.cut(test['1stFlrSF'], 6)\n",
    "for i in dummy_test.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: 2ndFlrSF, Second floor square feet\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: 25%\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: \n",
    "- **EDA**: Interestingly we see a highly positively correlated relationship with `SalePrice`, however we also see a significant number of houses with value = 0. We also see a high dependance and variation between neighborhoods, building types and lot sizes. It is evident that all the variables related to \"space\" are important in this analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"2ndFlrSF\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_train = pd.cut(train['2ndFlrSF'], 6)\n",
    "for i in dummy_train.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_test = pd.cut(test['2ndFlrSF'], 6)\n",
    "for i in dummy_test.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: LowQualFinSF, Low quality finished square feet (all floors)\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: 1%\n",
    "- **Type**: Numerical -> use a flag\n",
    "- **Encoding needed**: No\n",
    "- **Comments**:\n",
    "- **EDA**: We can see that there is a large number of properties with a value of 0 for this feature. Clearly, it does not have a significant correlation with SalePrice. For this reason, I will replace this feature with a flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"LowQualFinSF\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: GrLivArea, Above grade (ground) living area square feet\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: 60%\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: We can see that there are large values of `GrLivArea` that have low prices. These look liike outliers. We'll keep this in mind and we'll check them out under the `Detect outliers` section. There is also\n",
    "one in the test set but we obviously can't drop that one, otherwise your submission will be invalid. \n",
    "Can we use linear regression here? Ideally, if the assumptions are met, the residuals will be randomly scattered around the centerline of zero with no apparent pattern. The residual will look like an unstructured cloud of points centered around zero. However, our residual plot is anything but an unstructured cloud of points. Even though it seems like there is a linear relationship between the response variable and predictor variable, the residual plot looks more like a funnel. The error plot shows that as GrLivArea value increases, the variance also increases, which is the characteristics known as Heteroscedasticity. Credit to this [reference](https://www.kaggle.com/masumrumi/a-detailed-regression-guide-with-house-pricing/notebook).\n",
    "- **EDA**: We see a very high positive correlation with `SalePrice`. We also see the values varying very highly between styles of houses and neigborhood. We could create some bins and dummy features. The code snippet for this is reported under `feature_engineering`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"GrLivArea\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_train = pd.cut(train['GrLivArea'], 6)\n",
    "for i in dummy_train.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_test = pd.cut(test['GrLivArea'], 6)\n",
    "for i in dummy_test.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: BsmtFullBath, Basement full bathrooms\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**:\n",
    "- **EDA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"BsmtFullBath\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: BsmtHalfBath\n",
    "- **Missing value**: 2\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **COMMENTS** use zero for value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"BsmtHalfBath\", train, test, correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: FullBath, Full bathrooms above grade\n",
    "- **Missing value**: 0\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **COMMENTS** use zero for value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"FullBath\", train, test, correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: HalfBath, Half baths above grade\n",
    "- **Missing value**: 0\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **COMMENTS** use zero for value imputation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"HalfBath\", train, test, correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: KitchenQual\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical/Average\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "- **Missing value**: 1\n",
    "- **Cardinality**: low\n",
    "- **Type**: Categorical -> ordinal\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: use TA Typical/Average for missing value imputation\n",
    "- **EDA**: There is a clear positive correlation with the `SalePrice` and the quality of the kitchen. There is one value for \"Gd\" that has an extremely high SalePrice however."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"KitchenQual\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: TotRmsAbvGrd, Total rooms above grade (does not include bathrooms)\n",
    "- **Missing value**: 0\n",
    "- **Cardinality**: low\n",
    "- **Type**: Nuumerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: use TA Typical/Average for missing value imputation\n",
    "- **EDA**: Generally we see a positive correlation, as the number of rooms increases, so does the SalePrice. However due to low frequency, we do see some unreliable results for the very large and small values for this feature. Since this is a discrete numerical feature, I will leave it as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"TotRmsAbvGrd\", train, test, correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Functional, Home functionality (Assume typical unless deductions are warranted)\n",
    "       Typ\tTypical Functionality\n",
    "       Min1\tMinor Deductions 1\n",
    "       Min2\tMinor Deductions 2\n",
    "       Mod\tModerate Deductions\n",
    "       Maj1\tMajor Deductions 1\n",
    "       Maj2\tMajor Deductions 2\n",
    "       Sev\tSeverely Damaged\n",
    "       Sal\tSalvage only\n",
    "- **Missing value**: 2\n",
    "- **Cardinality**: low\n",
    "- **Type**: Categorical -> ordinal\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments** use mode for missing value imputation\n",
    "- **EDA**: This categorical feature shows that most houses have \"Typ\" functionality, and looking at the data description lead me to believe that there is an order within these categories, \"Typ\" being of the highest order. Therefore, I will replace the values of this feature by hand with numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Functional\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Fireplaces, Number of fireplaces\n",
    "- **Missing value**: 0\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: use mode for missing value imputation\n",
    "- **EDA**: Once again we have a positive correlation with SalePrice, with most houses having just 1 or 0 fireplaces. I will leave this feature as it is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Fireplaces\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: FireplaceQu\n",
    "\n",
    "       Ex\tExcellent - Exceptional Masonry Fireplace\n",
    "       Gd\tGood - Masonry Fireplace in main level\n",
    "       TA\tAverage - Prefabricated Fireplace in main living area or Masonry Fireplace in basement\n",
    "       Fa\tFair - Prefabricated Fireplace in basement\n",
    "       Po\tPoor - Ben Franklin Stove\n",
    "       NA\tNo Fireplace\n",
    "       \n",
    "- **Missing value**: 730\n",
    "- **Cardinality**: low\n",
    "- **Type**: Categorical, use Label Encoding and **NOT** one-hot encoding because order is important.\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: use NA-> No Fireplace for missing value imputation\n",
    "- **EDA**: We see a positive correlation and the fireplace quality increases. Most houses have either \"TA\" or \"Gd\" quality fireplaces. Since this is a categorical feature with order, I will replace the values by hand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"FireplaceQu\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: GarageType\n",
    "       2Types\tMore than one type of garage\n",
    "       Attchd\tAttached to home\n",
    "       Basment\tBasement Garage\n",
    "       BuiltIn\tBuilt-In (Garage part of house - typically has room above garage)\n",
    "       CarPort\tCar Port\n",
    "       Detchd\tDetached from home\n",
    "       NA\tNo Garage\n",
    "- **Missing value**: 76\n",
    "- **Cardinality**: low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: use NA-> No Garage for missing value imputation\n",
    "- **EDA**: Here we see \"BuiltIn\" and \"Attched\" having the 2 highest average SalePrices, with only a few extreme values within each class. Since this is categorical without order, I will create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"GarageType\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: GarageYrBlt: Year garage was built.\n",
    "- **Missing value**: 159\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: use mode only if if GarageType is different from NA? Otherwise, something must have gone wrong!\n",
    "- **EDA**: We can see a slight upward trend as the garage building year becomes more modern. We could create bins and then dummy variables on it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"GarageYrBlt\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_train = pd.cut(train['GarageYrBlt'], 3)\n",
    "for i in dummy_train.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_test = pd.cut(test['GarageYrBlt'], 3)\n",
    "for i in dummy_test.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: GarageFinish\n",
    "       Fin\tFinished\n",
    "       RFn\tRough Finished\t\n",
    "       Unf\tUnfinished\n",
    "       NA\tNo Garage\n",
    "- **Missing value**: 159\n",
    "- **Cardinality**: low\n",
    "- **Type**: Categorical -> use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: At the moment we are using NA for imputation but that needs to be cross-checked!\n",
    "- **EDA**: Here we see a nice split between the 3 classes, with \"Fin\" producing having the highest SalePrice's on average. I will create dummy variables for this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"GarageFinish\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: GarageCars\n",
    "- **Missing value**: 1\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: use 0.0 for missign values imputation\n",
    "- **EDA**: We generally see a positive correlation with an increasing garage car capacity. However, we see a slight dip for 4 cars I believe due to the low frequency of houses with a 4 car garage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"GarageCars\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: GarageArea\n",
    "- **Missing value**: 1\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: use 0.0 for missign values imputation\n",
    "- **EDA**: This has an extremely high positive correlation with SalePrice, and it is highly dependant on Neighborhood, building type and style of the house. This could be an important feature in the analysis, so I will bin this feature and create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"GarageArea\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: GarageQual\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical/Average\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "       NA\tNo Garage\n",
    "- **Missing value**: 159\n",
    "- **Cardinality**: low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: use NA -> No Garage for missign values imputation\n",
    "- **EDA**: We see a lot of homes having \"TA\" quality garages, with very few homes having high quality and low quality ones. I am going to cluster the classes here, and then create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"GarageQual\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: GarageCond\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tTypical/Average\n",
    "       Fa\tFair\n",
    "       Po\tPoor\n",
    "       NA\tNo Garage\n",
    "- **Missing value**: 159\n",
    "- **Cardinality**: low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: use NA -> No Garage for missign values imputation. GarageCond and GarageQual can be merged together?\n",
    "- **EDA**: We see a fairly similar pattern here with the previous feature. We see a slight positive correlation and then a dip, I believe due to the low number of houses that have \"Ex\" or \"Gd\" garage conditions. Similarly to before, I am going to cluster and then dummy (one-hot encoding) this feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"GarageCond\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: PavedDrive\n",
    "       Y\tPaved \n",
    "       P\tPartial Pavement\n",
    "       N\tDirt/Gravel\n",
    "- **Missing value**: 0\n",
    "- **Cardinality**: low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**:\n",
    "- **EDA**: Here we see the highest average price being demanded from houses with a paved driveway, and most houses in this area seem to have one. Since this is a categorical feature without order, I will create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"PavedDrive\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: WoodDeckSF, Wood deck area in square feet\n",
    "- **Missing value**: 0\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**:\n",
    "- **EDA**: This feature has a high positive correlation with SalePrice. We can also see that it varies widely with location, building type, style and size of the lot. There is a significant number of data points with a value of 0, so I will create a flag to indicate no Wood Deck. Then, since this is a continuous numeric feature, and I believe it to be an important one, I will bin this and then create dummy features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"WoodDeckSF\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_train = pd.cut(train['WoodDeckSF'], 4)\n",
    "for i in dummy_train.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_test = pd.cut(test['WoodDeckSF'], 4)\n",
    "for i in dummy_test.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: OpenPorchSF, Open porch area in square feet\n",
    "- **Missing value**: 0\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**:\n",
    "- **EDA**: We can see a high number of data points having a value of 0 here once again. Apart from this, we see a high positive correlation with SalePrice showing that this may be an influential factor for analysis. Finally, we see that this value ranges widely based on location, building type, style and lot. I will create a flag to indicate no open porch, then I will bin the feature and create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"OpenPorchSF\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_train = pd.cut(train['OpenPorchSF'], 4)\n",
    "for i in dummy_train.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "dummy_test = pd.cut(test['OpenPorchSF'], 4)\n",
    "for i in dummy_test.unique():\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: EnclosedPorch, Enclosed porch area in square feet\n",
    "- **Missing value**: 0\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments:** will combine this feature into a single `TotalPorchSF` feature\n",
    "- **EDA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"EnclosedPorch\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: 3SsnPorch, Three season porch area in square feet\n",
    "- **Missing value**: 0\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: will combine this feature into a single `TotalPorchSF` feature\n",
    "- **EDA**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"3SsnPorch\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: ScreenPorch, Screen porch area in square feet\n",
    "- **Missing value**: 0\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: will combine this feature into a single `TotalPorchSF` feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"ScreenPorch\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: PoolArea: Pool area in square feet\n",
    "- **Missing value**: 0\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: will combine this feature into a single `TotalPorchSF` feature\n",
    "- **EDA**: We see almost 0 correlation due to the high number of houses without a pool. Hence, I will create a flag here, and then we'll drop the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"PoolArea\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: PoolQC: Pool quality\n",
    "\t\t\n",
    "       Ex\tExcellent\n",
    "       Gd\tGood\n",
    "       TA\tAverage/Typical\n",
    "       Fa\tFair\n",
    "       NA\tNo Pool\n",
    "- **Missing value**: 2909\n",
    "- **Cardinality**: low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments** Use NA-> No Pool for data imputation as suggested by the `data_description.txt`\n",
    "- **EDA**: Due to not many houses having a pool, we see very low numbers of observations for each class. Since this does not hold much information this feature, I will simply remove it. Also consider what we discuss for `PoolArea` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"PoolQC\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: Fence quality\n",
    "    \t\t\n",
    "       GdPrv\tGood Privacy\n",
    "       MnPrv\tMinimum Privacy\n",
    "       GdWo\tGood Wood\n",
    "       MnWw\tMinimum Wood/Wire\n",
    "       NA\tNo Fence\n",
    "- **Missing value**: 2348\n",
    "- **Cardinality**: low\n",
    "- **Type**: Categorical -> use one-hot encoding\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Use NA-> No Fence for data imputation\n",
    "- **EDA**: Here we see that the houses with the most privacy have the highest average SalePrice. There seems to be a slight order within the classes, however some of the class descriptions are slightly ambiguous, therefore I will create dummy variables here from this categorical feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"Fence\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: MiscFeature: Miscellaneous feature not covered in other categories\n",
    "\t\t\n",
    "       Elev\tElevator\n",
    "       Gar2\t2nd Garage (if not described in garage section)\n",
    "       Othr\tOther\n",
    "       Shed\tShed (over 100 SF)\n",
    "       TenC\tTennis Court\n",
    "       NA\tNone\n",
    "- **Missing value**: 1408\n",
    "- **Cardinality**: low\n",
    "- **Type**: Categorical\n",
    "- **Encoding needed**: Yes\n",
    "- **Comments**: Use NA-> None for data imputation\n",
    "- **EDA**: We can see here that only a low number of houses in this area with any miscalleanous features. Hence, I do not believe that this feature holds much. Therefore I will drop this feature along with MiscVal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"MiscFeature\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: MiscVal: Value of miscellaneous feature\n",
    "- **Missing value**: 2814\n",
    "- **Cardinality**: low\n",
    "- **Type**: Numerical\n",
    "- **Encoding needed**: No\n",
    "- **COMMENTS**:\n",
    "- **EDA**: We can see here that only a low number of houses in this area with any miscalleanous features. Hence, I do not believe that this feature holds much. Therefore I will drop this feature along with MiscVal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "analyse_single_feature(\"MiscVal\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: MoSold: Month Sold (MM)\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Numerical but treated as categorical.\n",
    "- **Encoding needed**: No\n",
    "- **Comments**: Categorical data represent characteristics such as a person’s gender, marital status, hometown, or the types of movies they like. Categorical data can take on numerical values (such as “1” indicating male and “2” indicating female), but those numbers don’t have mathematical meaning. You couldn’t add them together, for example. (Other names for categorical data are qualitative data, or Yes/No data.)\n",
    "- **EDA**: Although this feature is a numeric feature, it should really be a category. We can see that there is no real indicator as to any months that consistetly sold houses of a higher price, however there seem to be a fairly even distribution of values between classes. I will create dummy variables from each category."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"MoSold\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Feature**: YrSold: Year Sold (YYYY)\n",
    "- **Missing value**: None\n",
    "- **Cardinality**: Low\n",
    "- **Type**: Numerical but treated as categorical.\n",
    "- **Encoding needed**: No\n",
    "- **Comments**:\n",
    "- **EDA**: Here we see just a 5 year time period of which the houses in this dataset were sold. There is an even distribution of values between each class, and each year has a very similar average SalePrice. Even though this is numeric, it should be categorical. Therefore I will create dummy variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "analyse_single_feature(\"YrSold\", train, test,\n",
    "                       correlated_feature=\"SalePrice\", target=\"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- People tend to move during the summer?\n",
    "- Is the trend consistent over the years?\n",
    "- [reference](https://www.kaggle.com/janiobachmann/house-prices-useful-regression-techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(16,8))\n",
    "sns.countplot(y=\"MoSold\", hue=\"YrSold\", data=train)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Which neighborhoods gave the most revenue?\n",
    "- This might indicate higher demand toward certain neighborhoods.\n",
    "- [reference](https://www.kaggle.com/janiobachmann/house-prices-useful-regression-techniques)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-white')\n",
    "zoning_value = train.groupby(by=['MSZoning'], as_index=False)[\n",
    "    'SalePrice'].sum()\n",
    "zoning = zoning_value['MSZoning'].values.tolist()\n",
    "\n",
    "\n",
    "# Let's create a pie chart.\n",
    "labels = ['C: Commercial', 'FV: Floating Village Res.', 'RH: Res. High Density', 'RL: Res. Low Density',\n",
    "          'RM: Res. Medium Density']\n",
    "total_sales = zoning_value['SalePrice'].values.tolist()\n",
    "explode = (0, 0, 0, 0.1, 0)\n",
    "\n",
    "fig, ax1 = plt.subplots(figsize=(12, 8))\n",
    "texts = ax1.pie(total_sales, explode=explode, autopct='%.1f%%', shadow=True, startangle=90, pctdistance=0.8,\n",
    "                radius=0.5)\n",
    "\n",
    "\n",
    "ax1.axis('equal')\n",
    "plt.title('Sales Groupby Zones', fontsize=16)\n",
    "plt.tight_layout()\n",
    "plt.legend(labels, loc='best')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-white')\n",
    "SalesbyZone = train.groupby(['YrSold','MSZoning']).SalePrice.count()\n",
    "SalesbyZone.unstack().plot(kind='bar',stacked=True, colormap= 'gnuplot',  \n",
    "                           grid=False,  figsize=(12,8))\n",
    "plt.title('Building Sales (2006 - 2010) by Zoning', fontsize=18)\n",
    "plt.ylabel('Sale Price', fontsize=14)\n",
    "plt.xlabel('Sales per Year', fontsize=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "sns.countplot(x=\"Neighborhood\", data=train, palette=\"Set2\")\n",
    "ax.set_title(\"Types of Neighborhoods\", fontsize=20)\n",
    "ax.set_xlabel(\"Neighborhoods\", fontsize=16)\n",
    "ax.set_ylabel(\"Number of Houses Sold\", fontsize=16)\n",
    "ax.set_xticklabels(labels=train['Neighborhood'].unique(),rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sawyer and SawyerW tend to be the most expensive neighberhoods. Nevertheless, what makes them the most expensive\n",
    "# Is it the LotArea or LotFrontage? Let's find out!\n",
    "fig, ax = plt.subplots(figsize=(12,8))\n",
    "ax = sns.boxplot(x=\"Neighborhood\", y=\"SalePrice\", data=train)\n",
    "ax.set_title(\"Range Value of the Neighborhoods\", fontsize=18)\n",
    "ax.set_ylabel('Price Sold', fontsize=16)\n",
    "ax.set_xlabel('Neighborhood', fontsize=16)\n",
    "ax.set_xticklabels(labels=train['Neighborhood'].unique(), rotation=45)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which Neighborhoods had the best Quality houses?\n",
    "plt.style.use('seaborn-white')\n",
    "types_foundations = train.groupby(['Neighborhood', 'OverallQual']).size()\n",
    "types_foundations.unstack().plot(kind='bar', stacked=True, colormap='RdYlBu', figsize=(13,11), grid=False)\n",
    "plt.ylabel('Overall Price of the House', fontsize=16)\n",
    "plt.xlabel('Neighborhood', fontsize=16)\n",
    "plt.xticks(rotation=90, fontsize=12)\n",
    "plt.title('Overall Quality of the Neighborhoods', fontsize=18)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Overall Condition**: of the house or building, meaning that further remodelations are likely to happen in the future, either for reselling or to accumulate value in their real-estate..\n",
    "- **Overall Quality**: The quality of the house is one of the factors that mostly impacts SalePrice. It seems that the overall material that is used for construction and the finish of the house has a great impact on SalePrice.\n",
    "- **Year Remodelation**: Houses in the high price range remodelled their houses sooner. The sooner the remodelation the higher the value of the house. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- What follows was take from [here](https://www.kaggle.com/ar2017/house-price-prediction-systematic-eda)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_boxen_count_2x2(df, first_feature, second_feature, figsize, feature_against):\n",
    "    \"\"\"\n",
    "    Reference\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Create boxen plot of first_feature and Log_SalePrice\n",
    "    ax3 = plt.subplot(223)\n",
    "    sns.boxenplot(x=first_feature, y=feature_against, data=df, color='dimgrey')\n",
    "\n",
    "    # Create boxen plot of second_feature and Log_SalePrice\n",
    "    ax4 = plt.subplot(224, sharey=ax3)\n",
    "    sns.boxenplot(x=second_feature, y=feature_against, data=df, color='tomato')\n",
    "    plt.setp(ax4.get_yticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "\n",
    "    # Create countplot of Condition1\n",
    "    ax1 = plt.subplot(221, sharex=ax3)\n",
    "    sns.countplot(x=first_feature, data=df, color='dimgrey')\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    plt.xlabel('')\n",
    "\n",
    "    # Create countplot of Condition2\n",
    "    ax2 = plt.subplot(222, sharey=ax1, sharex=ax4)\n",
    "    sns.countplot(x=second_feature, data=df, color='tomato')\n",
    "    plt.setp(ax2.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "    plt.xlabel('')\n",
    "\n",
    "    # Adjusting the spaces between graphs\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "create_boxen_count_2x1(train, 'MSZoning', (16,7), \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- One one way to encode this is as follows. We'll make a copy for now and then do it properly under the `feature_engineering` section.\n",
    "- In this way the feature also plotted in order of importance (from high to low).\n",
    "- Most of the houses are located in zone number 2 (residential low population density area) and Houses located at lower population density area generally have higher `SalePrice` than houses at higher population density area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "center"
   },
   "outputs": [],
   "source": [
    "MSZoning_map = {'FV':1, 'RL':2, 'RM':3, 'RH':4, 'C (all)':5}\n",
    "train_copy = copy.deepcopy(train)\n",
    "train_copy['MSZoning'].replace(MSZoning_map, inplace=True)\n",
    "\n",
    "create_boxen_count_2x1(train_copy, 'MSZoning', (16,7), \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Price per square feet? Can we use this and plot against the neighboorhood to make a classification of the best neighborhood?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxen_count_2x1(train, 'Neighborhood', (16,7), \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In this cell I'd just want to highlight how the correlation changes depending on which encoding we use:\n",
    "    - If we use a label encoding, we'll keep one column but we assign a different value to each entries.\n",
    "    - If we use one-hot encoding, we'll add one column for each entries in in the columns.\n",
    "    \n",
    "- The results tell us two interesting things:\n",
    "    - If we use label encoding we get the general correlation of the feature wrt the target. In this case we can see that if we label encode the data, we keep the relative ordinal importance of the feature, and the feature turn out to be slightly negatively correlated against the target variable.\n",
    "    - On the other hand, if we hot-encode the feature the model is unable to rank the instance under the same feature but we get an extra piece of information. We see that some of the entries are instead positively correlated against the target. Nevertheless, we can still see the most of them are negatively correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MSZoning_map"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "train_copy = copy.deepcopy(train)\n",
    "train_copy['MSZoning'].replace(MSZoning_map, inplace=True)\n",
    "\n",
    "surround_feats = train_copy[['MSZoning', 'SalePrice']]\n",
    "sns.heatmap(surround_feats.corr(), annot=True, cmap='RdBu')\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "train_copy2 = copy.deepcopy(train)\n",
    "\n",
    "surround_feats = train_copy2[['MSZoning', 'SalePrice']]\n",
    "surround_feats = pd.get_dummies(surround_feats)\n",
    "\n",
    "sns.heatmap(surround_feats.corr(), annot=True, cmap='RdBu')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Now, we'd like to answer another question. **How is the correlation affected if we log the target?**\n",
    "- The answer is that by logging the target we get (*for some, not for all!*) an increase in the absolute value of the correlation meaning:\n",
    "    - Those that were positevely correlated are now even more so.\n",
    "    - Those that were negatively correlated are now even more so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "train_copy = copy.deepcopy(train)\n",
    "train_copy['MSZoning'].replace(MSZoning_map, inplace=True)\n",
    "train_copy['SalePrice'] = np.log1p(train_copy['SalePrice'])\n",
    "\n",
    "surround_feats = train_copy[['MSZoning', 'SalePrice']]\n",
    "sns.heatmap(surround_feats.corr(), annot=True, cmap='RdBu')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_style": "split"
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6, 6))\n",
    "train_copy2 = copy.deepcopy(train)\n",
    "train_copy2['SalePrice'] = np.log1p(train_copy2['SalePrice'])\n",
    "\n",
    "surround_feats = train_copy2[['MSZoning', 'SalePrice']]\n",
    "surround_feats = pd.get_dummies(surround_feats)\n",
    "\n",
    "sns.heatmap(surround_feats.corr(), annot=True, cmap='RdBu')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxen_count_2x2(train, 'Condition1', 'Condition2', (16,7), \"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_boxen_count_2x3(df,first_feature, second_feature, third_feature, figsize, feature_against):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Create boxenplot of first_feature and Log_SalePrice\n",
    "    ax4 = plt.subplot(234)\n",
    "    sns.boxenplot(x=first_feature, y=feature_against, data=df, color='dimgrey')\n",
    "\n",
    "    # Create boxenplot of second_feature and Log_SalePrice\n",
    "    ax5 = plt.subplot(235, sharey=ax4)\n",
    "    sns.boxenplot(x=second_feature, y=feature_against, data=df, color='tomato')\n",
    "    plt.setp(ax5.get_yticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "\n",
    "    # Create boxenplot of third_feature and Log_SalePrice\n",
    "    ax6 = plt.subplot(236, sharey=ax4)\n",
    "    sns.boxenplot(x=third_feature, y=feature_against, data=df, color='darkseagreen')\n",
    "    plt.setp(ax6.get_yticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "\n",
    "    #---------------------------------------------------------------------------------------\n",
    "\n",
    "    # Create countplot of first_feature\n",
    "    ax1 = plt.subplot(231, sharex=ax4)\n",
    "    sns.countplot(x=first_feature, data=df, color='dimgrey')\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    plt.xlabel('')\n",
    "\n",
    "    # Create countplot of second_feature\n",
    "    ax2 = plt.subplot(232, sharey=ax1, sharex=ax5)\n",
    "    sns.countplot(x=second_feature, data=df, color='tomato')\n",
    "    plt.setp(ax2.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "    plt.xlabel('')\n",
    "\n",
    "    # Create countplot of second_feature\n",
    "    ax3 = plt.subplot(233, sharey=ax1, sharex=ax6)\n",
    "    sns.countplot(x=third_feature, data=df, color='darkseagreen')\n",
    "    plt.setp(ax3.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "    plt.xlabel('')\n",
    "\n",
    "    # Adjusting the spaces between graphs\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxen_count_2x3(train,'MSSubClass', 'BldgType', 'HouseStyle', (16,8), \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There seems to be no relationship between MSSubClass and SalePrice.\n",
    "- There seems to be no relationship between BldgType and SalePrice.\n",
    "- There seems to be no relationship between HouseStyle and SalePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxen_count_2x3(train, 'MSSubClass', 'BldgType', 'HouseStyle', (16,8), \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Houses with higher OverallQual have higher Log_SalePrice. In other words, there seems to be a strong relationship between OverallQual and Log_SalePrice.\n",
    "- There seems to be no relationship between OverallCond and Log_SalePrice. Similarly, there seems to be no relationship between Functional and Log_SalePrice. These features are not likely to help in predicting house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxen_count_2x3(train, 'OverallQual', 'OverallCond', 'Functional', (16,8), \"SalePrice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "code_folding": [
     0
    ]
   },
   "outputs": [],
   "source": [
    "def create_boxen_count_2x4(df, first_feature, second_feature, third_feature, fourth_feature, figsize, feature_against):\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # Create boxenplot of first_feature and Log_SalePrice\n",
    "    ax5 = plt.subplot(245)\n",
    "    sns.boxenplot(x=first_feature, y=feature_against,\n",
    "                  data=df, color='dimgrey')\n",
    "\n",
    "    # Create boxenplot of second_feature and Log_SalePrice\n",
    "    ax6 = plt.subplot(246, sharey=ax5)\n",
    "    sns.boxenplot(x=second_feature, y=feature_against,\n",
    "                  data=df, color='tomato')\n",
    "    plt.setp(ax6.get_yticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "\n",
    "    # Create boxenplot of third_feature and Log_SalePrice\n",
    "    ax7 = plt.subplot(247, sharey=ax5)\n",
    "    sns.boxenplot(x=third_feature, y=feature_against,\n",
    "                  data=df, color='darkseagreen')\n",
    "    plt.setp(ax7.get_yticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "\n",
    "    # Create boxenplot of fourth_feature and Log_SalePrice\n",
    "    ax8 = plt.subplot(248, sharey=ax5)\n",
    "    sns.boxenplot(x=fourth_feature, y=feature_against,\n",
    "                  data=df, color='seagreen')\n",
    "    plt.setp(ax8.get_yticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "\n",
    "    # ---------------------------------------------------------------------------------------\n",
    "\n",
    "    # Create countplot of first_feature\n",
    "    ax1 = plt.subplot(241, sharex=ax5)\n",
    "    sns.countplot(x=first_feature, data=df, color='dimgrey')\n",
    "    plt.setp(ax1.get_xticklabels(), visible=False)\n",
    "    plt.xlabel('')\n",
    "\n",
    "    # Create countplot of second_feature\n",
    "    ax2 = plt.subplot(242, sharey=ax1, sharex=ax6)\n",
    "    sns.countplot(x=second_feature, data=df, color='tomato')\n",
    "    plt.setp(ax2.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax2.get_xticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "    plt.xlabel('')\n",
    "\n",
    "    # Create countplot of third_feature\n",
    "    ax3 = plt.subplot(243, sharey=ax1, sharex=ax7)\n",
    "    sns.countplot(x=third_feature, data=df, color='darkseagreen')\n",
    "    plt.setp(ax3.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax3.get_xticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "    plt.xlabel('')\n",
    "\n",
    "    # Create countplot of fourth_feature\n",
    "    ax4 = plt.subplot(244, sharey=ax1, sharex=ax8)\n",
    "    sns.countplot(x=fourth_feature, data=df, color='seagreen')\n",
    "    plt.setp(ax4.get_yticklabels(), visible=False)\n",
    "    plt.setp(ax4.get_xticklabels(), visible=False)\n",
    "    plt.ylabel('')\n",
    "    plt.xlabel('')\n",
    "\n",
    "    # Adjusting the spaces between graphs\n",
    "    plt.subplots_adjust(wspace=0, hspace=0)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There seems to be no relationship between lot characteristics related features and Log_SalePrice.\n",
    "- These features are not likely to help in predicting house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_boxen_count_2x4(train,'LotShape', 'LandContour', 'LotConfig', 'LandSlope', (20,8), \"SalePrice\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- I need to find a better place to put this plot!\n",
    "- A joint plot does two things:\n",
    "    - Scatter plot the data, and fit a regression line\n",
    "    - Show a bar plor and its kde on the side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.jointplot(x='MasVnrArea', \n",
    "              y='SalePrice', \n",
    "              data=train, \n",
    "              kind='reg', \n",
    "              height=9,\n",
    "              color='darkseagreen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EDA's conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- There are features with **ambiguous types**. `GarageYrBlt`, `MoSold`, `YearBuilt`, `YearRemodAdd` and `YrSold` are date features. Those are numerical features it might be better to use some of them as categorical features.\n",
    "\n",
    "- There were a lot of features with **missing entries** which made them sparse. \n",
    "\n",
    "- Target distribution is **highly skewed** and long tailed because of the outliers. It requires a transformation in order to perform better in models. Dealing with the outliers could also achieve better model performance.\n",
    "\n",
    "- Many features are **strongly correlated with each other** and target. This relationship can be used to create new features with feature interaction in order to overcome multicollinearity issue.\n",
    "\n",
    "- Some numerical features have **too many zeros**, something that needs to be addressed especially if a log transformation is to be use. \n",
    "\n",
    "- Some categorical features are **not informative** for two reasons. The feature is either too homogenous like Utilities feature, or all of the values have the same characteristics like MoSold feature. Those features can be conbined with other features or dropped completely.\n",
    "\n",
    "- There are some numerical feature distributions that are **too noisy** and therefore bring little value to the model is modelle.\n",
    "\n",
    "- Some features have have a **different distributions** in training and test set are quite different. They may require grouping to overcome this problem."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# This is just a small reminder of the numerical vs. categorical features in the two datasets\n",
    "_,_ = get_features_type(train)\n",
    "_,_ = get_features_type(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "trainingAI",
   "language": "python",
   "name": "trainingai"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
